{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-12T16:44:01.130809Z","iopub.execute_input":"2023-07-12T16:44:01.131470Z","iopub.status.idle":"2023-07-12T16:44:01.144213Z","shell.execute_reply.started":"2023-07-12T16:44:01.131421Z","shell.execute_reply":"2023-07-12T16:44:01.142912Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nfrom funcy import print_durations","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:32:43.565321Z","iopub.execute_input":"2023-07-12T18:32:43.565689Z","iopub.status.idle":"2023-07-12T18:32:43.573495Z","shell.execute_reply.started":"2023-07-12T18:32:43.565657Z","shell.execute_reply":"2023-07-12T18:32:43.572498Z"},"trusted":true},"execution_count":159,"outputs":[]},{"cell_type":"markdown","source":"### Fetch article","metadata":{}},{"cell_type":"code","source":"!pip install wikipedia","metadata":{"execution":{"iopub.status.busy":"2023-07-12T16:46:11.503643Z","iopub.execute_input":"2023-07-12T16:46:11.504013Z","iopub.status.idle":"2023-07-12T16:46:26.036108Z","shell.execute_reply.started":"2023-07-12T16:46:11.503957Z","shell.execute_reply":"2023-07-12T16:46:26.034662Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting wikipedia\n  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from wikipedia) (4.12.2)\nRequirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wikipedia) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.5.7)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.3.2.post1)\nBuilding wheels for collected packages: wikipedia\n  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11696 sha256=35bca2dc19216449116e0b3ce33db17a25c34cd0ef71f1ead37e849b46cc4831\n  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\nSuccessfully built wikipedia\nInstalling collected packages: wikipedia\nSuccessfully installed wikipedia-1.4.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import wikipedia\n\n@print_durations\ndef get_text_of_wiki_page(article_name: str):\n    \"\"\"Given an article name(need not be exact title of page),\n    return the textual content of the wikipedia article.\n    We do a search for the articles and select the top-1 result, in case\n    where the article name is not the exact title.\n\n    Args:\n        article_name (str): Name of a wikipedia article\n\n    Returns:\n        str: The text of that article.\n    \"\"\"\n    article_name_result = wikipedia.page(wikipedia.search(article_name)[0], auto_suggest=False)\n    article_name_content = article_name_result.content\n    article_name_content.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n    return article_name_content","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:32:52.831192Z","iopub.execute_input":"2023-07-12T18:32:52.831554Z","iopub.status.idle":"2023-07-12T18:32:52.838121Z","shell.execute_reply.started":"2023-07-12T18:32:52.831524Z","shell.execute_reply":"2023-07-12T18:32:52.837064Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"code","source":"tea = get_text_of_wiki_page(\"Tea\")","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:32:56.040121Z","iopub.execute_input":"2023-07-12T18:32:56.041239Z","iopub.status.idle":"2023-07-12T18:32:56.496355Z","shell.execute_reply.started":"2023-07-12T18:32:56.041191Z","shell.execute_reply":"2023-07-12T18:32:56.495250Z"},"trusted":true},"execution_count":161,"outputs":[{"name":"stdout","text":"  450.08 ms in get_text_of_wiki_page('Tea')\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download(\"punkt\")\nfrom nltk import sent_tokenize\nfrom nltk.tokenize import word_tokenize","metadata":{"execution":{"iopub.status.busy":"2023-07-12T17:01:21.034324Z","iopub.execute_input":"2023-07-12T17:01:21.034728Z","iopub.status.idle":"2023-07-12T17:01:21.042413Z","shell.execute_reply.started":"2023-07-12T17:01:21.034693Z","shell.execute_reply":"2023-07-12T17:01:21.041165Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"tea_sentences = sent_tokenize(tea)\nlen(tea_sentences)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T16:48:43.521578Z","iopub.execute_input":"2023-07-12T16:48:43.522077Z","iopub.status.idle":"2023-07-12T16:48:43.574453Z","shell.execute_reply.started":"2023-07-12T16:48:43.522029Z","shell.execute_reply":"2023-07-12T16:48:43.573535Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"280"},"metadata":{}}]},{"cell_type":"code","source":"def annotate_sentence(sentence, mention):\n    match = re.search(mention.lower(), sentence.lower())\n    start, end = match.span()\n    sentence = sentence[:start] + \" [START_ENT] \" + sentence[start:end] + \" [END_ENT] \" + sentence[end:]\n    return sentence","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:35:15.530955Z","iopub.execute_input":"2023-07-12T18:35:15.531939Z","iopub.status.idle":"2023-07-12T18:35:15.539955Z","shell.execute_reply.started":"2023-07-12T18:35:15.531899Z","shell.execute_reply":"2023-07-12T18:35:15.538959Z"},"trusted":true},"execution_count":167,"outputs":[]},{"cell_type":"code","source":"annotate_sentence(tea_sentences[0], \"China\"), annotate_sentence(tea_sentences[0], \"Myanmar\")","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:33:06.075687Z","iopub.execute_input":"2023-07-12T18:33:06.076164Z","iopub.status.idle":"2023-07-12T18:33:06.085887Z","shell.execute_reply.started":"2023-07-12T18:33:06.076121Z","shell.execute_reply":"2023-07-12T18:33:06.084614Z"},"trusted":true},"execution_count":163,"outputs":[{"name":"stdout","text":"   13.78 mks in annotate_sentence('Tea is an aromatic be..., 'China')\n    9.97 mks in annotate_sentence('Tea is an aromatic be..., 'Myanmar')\n","output_type":"stream"},{"execution_count":163,"output_type":"execute_result","data":{"text/plain":"('Tea is an aromatic beverage prepared by pouring hot or boiling water over cured or fresh leaves of Camellia sinensis, an evergreen shrub native to East Asia which probably originated in the borderlands of southwestern  [START_ENT] China [END_ENT]  and northern Myanmar.',\n 'Tea is an aromatic beverage prepared by pouring hot or boiling water over cured or fresh leaves of Camellia sinensis, an evergreen shrub native to East Asia which probably originated in the borderlands of southwestern China and northern  [START_ENT] Myanmar [END_ENT] .')"},"metadata":{}}]},{"cell_type":"markdown","source":"### REBEL","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:27:49.730851Z","iopub.execute_input":"2023-07-12T18:27:49.731540Z","iopub.status.idle":"2023-07-12T18:28:03.794478Z","shell.execute_reply.started":"2023-07-12T18:27:49.731502Z","shell.execute_reply":"2023-07-12T18:28:03.793397Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"import ast\nimport torch\nimport pandas as pd\ngen_kwargs = {\n    \"max_length\": 256,\n    \"length_penalty\": 0,\n    \"num_beams\": 10,\n    \"num_return_sequences\": 10,\n}\n\n# Function to parse the generated text and extract the triplets\ndef extract_triplets(text):\n    triplets = []\n    relation, subject, relation, object_ = '', '', '', ''\n    text = text.strip()\n    current = 'x'\n    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n        if token == \"<triplet>\":\n            current = 't'\n            if relation != '':\n                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n                relation = ''\n            subject = ''\n        elif token == \"<subj>\":\n            current = 's'\n            if relation != '':\n                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n            object_ = ''\n        elif token == \"<obj>\":\n            current = 'o'\n            relation = ''\n        else:\n            if current == 't':\n                subject += ' ' + token\n            elif current == 's':\n                object_ += ' ' + token\n            elif current == 'o':\n                relation += ' ' + token\n    if subject != '' and relation != '' and object_ != '':\n        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n    return triplets\n\n@print_durations\ndef extract_relations_rebel(model, tokenizer, text):\n    \n#     tokenized_sentences = sentence_tokenize(text)\n    tokenized_sentences = [text]\n    list_triples = []\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    for text in tokenized_sentences:\n        model_inputs = tokenizer(text, max_length=256, padding=True, truncation=True, return_tensors = 'pt')\n        # Generate\n        generated_tokens = model.generate(\n            model_inputs[\"input_ids\"].to(model.device),\n            attention_mask=model_inputs[\"attention_mask\"].to(model.device),\n            **gen_kwargs,\n        )\n\n        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n\n        l1 = []\n        for idx, sentence in enumerate(decoded_preds):\n            #print(f'Prediction triplets sentence {idx}')\n            #print(extract_triplets(sentence))\n            l1 += extract_triplets(sentence)\n\n        d1 = {}\n        ctr = 0\n\n\n        for x in l1:\n            ctr += 1\n            if not str(x) in d1:\n                d1[str(x)] = 0\n            d1[str(x)] += 1\n\n        for x in d1:\n            t = x.replace(\"}\", \"\")\n            final_dict = t + \", 'Confidence': \" + str(d1[x]/ctr) + \"}\"\n            #print(final_dict)\n            final_dictionary = ast.literal_eval(final_dict)\n            list_triples.append(final_dictionary)\n\n    return pd.DataFrame(list_triples).sort_values(by=\"Confidence\", ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:35:26.560948Z","iopub.execute_input":"2023-07-12T18:35:26.561344Z","iopub.status.idle":"2023-07-12T18:35:26.579123Z","shell.execute_reply.started":"2023-07-12T18:35:26.561312Z","shell.execute_reply":"2023-07-12T18:35:26.577955Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"code","source":"tea_sentences[0], sent_tokenize(tea_sentences[0])","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:44:59.763040Z","iopub.execute_input":"2023-07-12T18:44:59.763402Z","iopub.status.idle":"2023-07-12T18:44:59.772376Z","shell.execute_reply.started":"2023-07-12T18:44:59.763372Z","shell.execute_reply":"2023-07-12T18:44:59.771333Z"},"trusted":true},"execution_count":173,"outputs":[{"execution_count":173,"output_type":"execute_result","data":{"text/plain":"('Tea is an aromatic beverage prepared by pouring hot or boiling water over cured or fresh leaves of Camellia sinensis, an evergreen shrub native to East Asia which probably originated in the borderlands of southwestern China and northern Myanmar.',\n ['Tea is an aromatic beverage prepared by pouring hot or boiling water over cured or fresh leaves of Camellia sinensis, an evergreen shrub native to East Asia which probably originated in the borderlands of southwestern China and northern Myanmar.'])"},"metadata":{}}]},{"cell_type":"code","source":"tea_0_triples = extract_relations_rebel(model=model, tokenizer=tokenizer, text=tea_sentences[0])\ntea_0_triples","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:35:28.469608Z","iopub.execute_input":"2023-07-12T18:35:28.469968Z","iopub.status.idle":"2023-07-12T18:35:29.586863Z","shell.execute_reply.started":"2023-07-12T18:35:28.469935Z","shell.execute_reply":"2023-07-12T18:35:29.585844Z"},"trusted":true},"execution_count":169,"outputs":[{"name":"stdout","text":"    1.10 s in extract_relations_rebel(model=BartForConditionalGene..., tokenizer=BartTokenizerFast(name..., text='Tea is an aromatic be...)\n","output_type":"stream"},{"execution_count":169,"output_type":"execute_result","data":{"text/plain":"        head                        type       tail  Confidence\n1      China          shares border with    Myanmar    0.222222\n3    Myanmar          shares border with      China    0.222222\n0      China                     part of  East Asia    0.200000\n4  East Asia                    has part      China    0.133333\n2    Myanmar                     part of  East Asia    0.088889\n5    Myanmar  located on terrain feature  East Asia    0.066667\n6  East Asia                    has part    Myanmar    0.044444\n7        Tea                 subclass of   beverage    0.022222","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>head</th>\n      <th>type</th>\n      <th>tail</th>\n      <th>Confidence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>China</td>\n      <td>shares border with</td>\n      <td>Myanmar</td>\n      <td>0.222222</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Myanmar</td>\n      <td>shares border with</td>\n      <td>China</td>\n      <td>0.222222</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>China</td>\n      <td>part of</td>\n      <td>East Asia</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>East Asia</td>\n      <td>has part</td>\n      <td>China</td>\n      <td>0.133333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Myanmar</td>\n      <td>part of</td>\n      <td>East Asia</td>\n      <td>0.088889</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Myanmar</td>\n      <td>located on terrain feature</td>\n      <td>East Asia</td>\n      <td>0.066667</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>East Asia</td>\n      <td>has part</td>\n      <td>Myanmar</td>\n      <td>0.044444</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Tea</td>\n      <td>subclass of</td>\n      <td>beverage</td>\n      <td>0.022222</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### GENRE","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ngenre_tokenizer = AutoTokenizer.from_pretrained(\"facebook/genre-linking-blink\")\ngenre_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/genre-linking-blink\").eval()","metadata":{"execution":{"iopub.status.busy":"2023-07-12T16:59:06.990338Z","iopub.execute_input":"2023-07-12T16:59:06.990739Z","iopub.status.idle":"2023-07-12T16:59:48.920001Z","shell.execute_reply.started":"2023-07-12T16:59:06.990707Z","shell.execute_reply":"2023-07-12T16:59:48.919018Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c50e1285c6114094841d56722b326379"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/999k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c87c05e4a88434b86c95fb37db2448d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43777e25c7fe431c8d856a0500dacc13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"213a17b2462347e6a19ed3384ac5f62d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c34eafedcc24b2b92cbddf94a33fde9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/997 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29142db746ef4e268b6e99d799d27a75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbdcafed2855492c9d0a2bcd4159a529"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/196 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1004ca5112044bc92fa602f6f1ae026"}},"metadata":{}}]},{"cell_type":"code","source":"def EL_GENRE(annotated_sentences, model, tokenizer):\n    \"\"\"A method to perform entity linking for entity-mentions annotated\n    in sentences using the GENRE model.\n\n    ```\n    tokenizer = AutoTokenizer.from_pretrained(\"facebook/genre-linking-blink\")\n    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/genre-linking-blink\").eval()\n\n    sentences = [\n        \"[START_ENT] England [END_ENT] won the cricket world cup in 2019\",\n        \"I just finished reading [START_ENT] 'The Jungle Book' [END_ENT]\",\n        \"India is a country in Asia. [START_ENT] It [END_ENT] has a rich cultural heritage\"\n    ]\n\n    EL_GENRE(annotated_sentences=sentences, model=model, tokenizer=tokenizer)\n    \n    ```\n\n    Args:\n        annotated_sentences (list): A list of sentences annotated with entity-mentions\n        model : GENRE model from huggingface hub\n        tokenizer : Appropriate tokenizer for GENRE model\n    \"\"\"\n    outputs = model.generate(\n    **tokenizer(annotated_sentences, return_tensors=\"pt\", padding=True),\n    num_beams=5,\n    num_return_sequences=1,\n    # OPTIONAL: use constrained beam search\n    # prefix_allowed_tokens_fn=lambda batch_id, sent: trie.get(sent.tolist()),\n    )\n\n    entites = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    # These entites are in the form of wikipedia page titles. Need to \n    # add the https://dbpedia/resource to each of them as postprocessing step\n    return entites","metadata":{"execution":{"iopub.status.busy":"2023-07-12T17:08:38.071967Z","iopub.execute_input":"2023-07-12T17:08:38.072913Z","iopub.status.idle":"2023-07-12T17:08:38.080399Z","shell.execute_reply.started":"2023-07-12T17:08:38.072865Z","shell.execute_reply":"2023-07-12T17:08:38.079282Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"sub, pred, obj, score = tea_0_triples.iloc[0].values","metadata":{"execution":{"iopub.status.busy":"2023-07-12T17:11:06.283726Z","iopub.execute_input":"2023-07-12T17:11:06.284114Z","iopub.status.idle":"2023-07-12T17:11:06.289170Z","shell.execute_reply.started":"2023-07-12T17:11:06.284080Z","shell.execute_reply":"2023-07-12T17:11:06.288203Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"sub_ent = EL_GENRE(annotate_sentence(tea_sentences[0], sub), genre_model, genre_tokenizer)\nobj_ent = EL_GENRE(annotate_sentence(tea_sentences[0], obj), genre_model, genre_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:28:14.878524Z","iopub.execute_input":"2023-07-12T18:28:14.878888Z","iopub.status.idle":"2023-07-12T18:28:18.760966Z","shell.execute_reply.started":"2023-07-12T18:28:14.878856Z","shell.execute_reply":"2023-07-12T18:28:18.759926Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"sub_ent, obj_ent","metadata":{"execution":{"iopub.status.busy":"2023-07-12T17:12:44.856518Z","iopub.execute_input":"2023-07-12T17:12:44.856877Z","iopub.status.idle":"2023-07-12T17:12:44.864277Z","shell.execute_reply.started":"2023-07-12T17:12:44.856847Z","shell.execute_reply":"2023-07-12T17:12:44.863317Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"(['China'], ['Myanmar'])"},"metadata":{}}]},{"cell_type":"markdown","source":"### Onto-Embeddings","metadata":{}},{"cell_type":"code","source":"!pip3 install sentence-transformers gensim SPARQLWrapper","metadata":{"execution":{"iopub.status.busy":"2023-07-12T17:21:59.314686Z","iopub.execute_input":"2023-07-12T17:21:59.315272Z","iopub.status.idle":"2023-07-12T17:22:15.536823Z","shell.execute_reply.started":"2023-07-12T17:21:59.315237Z","shell.execute_reply":"2023-07-12T17:22:15.535446Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting sentence-transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (4.3.1)\nCollecting SPARQLWrapper\n  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.30.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.65.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.15.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.23.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.16.4)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (6.3.0)\nCollecting rdflib>=6.1.1 (from SPARQLWrapper)\n  Downloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.6.3)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\nCollecting isodate<0.7.0,>=0.6.0 (from rdflib>=6.1.1->SPARQLWrapper)\n  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyparsing<4,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=15d96ab15126a91fae9979d9f7ff3ed6a5cddee440b4d75bdf9b183bbc03d91c\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\nSuccessfully built sentence-transformers\nInstalling collected packages: isodate, rdflib, SPARQLWrapper, sentence-transformers\nSuccessfully installed SPARQLWrapper-2.0.0 isodate-0.6.1 rdflib-6.3.2 sentence-transformers-2.2.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nencoder_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\ndef get_sentence_transformer_model(model_name):\n    model = SentenceTransformer(model_name_or_path=model_name)\n    return model\n\n@print_durations\ndef get_embeddings(labels, sent_tran_model):\n    embeddings = sent_tran_model.encode(labels, show_progress_bar=False)\n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:35:52.025021Z","iopub.execute_input":"2023-07-12T18:35:52.025421Z","iopub.status.idle":"2023-07-12T18:35:52.368065Z","shell.execute_reply.started":"2023-07-12T18:35:52.025391Z","shell.execute_reply":"2023-07-12T18:35:52.367053Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"code","source":"from SPARQLWrapper import SPARQLWrapper, JSON\n\nNS_RESOURCE = 'http://dbpedia.org/resource/'\nNS_RESOURCE_LEN = len(NS_RESOURCE)\n\nNS_ONTOLOGY = 'http://dbpedia.org/ontology/'\nNS_ONTOLOGY_LEN = len(NS_ONTOLOGY)\n\n\ndef retrieve_tbox(lang='en', offset=0):\n    sparql = SPARQLWrapper('http://dbpedia.org/sparql')\n    query = f\"\"\"\n    SELECT ?uri ?label {{\n      ?uri a ?type ; rdfs:label ?label .\n      values(?type) {{ (owl:Class) (rdf:Property) }}\n      filter(lang(?label) = '{lang}' && regex(?uri, \"http://dbpedia.org/ontology/\"))\n    }} LIMIT 10000 OFFSET {offset}\n    \"\"\"\n    sparql.setQuery(query)\n    sparql.setReturnFormat(JSON)\n    results = sparql.query().convert()\n    \n    tbox = {}\n    for result in results['results']['bindings']:\n        uri = result['uri']['value']\n        label = result['label']['value']\n        if label not in tbox:\n            tbox[label] = set()\n        tbox[label].add(uri)\n    return tbox\n\ndef get_labels_and_tbox():\n    offset = 0\n    tbox = {}\n    while True:\n        tbox_chunk = retrieve_tbox(lang='en', offset=offset)\n        if len(tbox_chunk) == 0:\n            break\n        offset += 10000\n        for k, v in tbox_chunk.items():\n            if k not in tbox:\n                tbox[k] = set()\n            tbox[k] = tbox[k].union(v)\n    labels = [l.replace('\\n', ' ') for l in tbox]\n    return labels, tbox\n\ndef to_uri(label, tbox):\n    return list(filter(lambda x: 'A' <= x[NS_ONTOLOGY_LEN : NS_ONTOLOGY_LEN+1] <= 'z', tbox[label]))\n\ndef write_embeddings_to_file(embeddings, labels, filename):\n    with open(filename, 'w', encoding='utf-8') as f_out:\n        f_out.write(f\"{len(labels)} {len(embeddings[0])}\\n\")\n        for label, embedding in zip(labels, embeddings):\n            f_out.write(f\"{label.replace(' ', '_')} {' '.join([str(x) for x in embedding])}\\n\")\n    print(\"Embeddings written to file successfully\")","metadata":{"execution":{"iopub.status.busy":"2023-07-12T17:28:14.011805Z","iopub.execute_input":"2023-07-12T17:28:14.012215Z","iopub.status.idle":"2023-07-12T17:28:14.025769Z","shell.execute_reply.started":"2023-07-12T17:28:14.012180Z","shell.execute_reply":"2023-07-12T17:28:14.024632Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"from gensim.models import KeyedVectors\n\n\ndef ontosim_search(term, gensim_model, sent_tran_model, tbox):\n    result = gensim_model.most_similar(\n        positive=sent_tran_model.encode([term], show_progress_bar=False), topn=5)\n    out = []\n    for label, score in result:\n        out.append({'label': label.replace('_', ' '), 'score': score})\n    df = pd.DataFrame(out)\n    df.insert(0, 'URIs', df['label'].map(lambda x: to_uri(x, tbox=tbox)))\n    return df\n\ndef load_gensim_model_from_file(filepath):\n    model = KeyedVectors.load_word2vec_format(filepath, binary=False)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:26:32.260925Z","iopub.execute_input":"2023-07-12T18:26:32.261332Z","iopub.status.idle":"2023-07-12T18:26:32.269192Z","shell.execute_reply.started":"2023-07-12T18:26:32.261297Z","shell.execute_reply":"2023-07-12T18:26:32.268062Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"import time\nstart = time.time()\nlabels, tbox = get_labels_and_tbox()\nembeddings = get_embeddings(labels, encoder_model)\nwrite_embeddings_to_file(embeddings, labels, \"dbpedia-ontology.vectors\")\nend = time.time()\nprint(f\"Time taken to compute and write embeddings - {end-start} seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-07-12T17:28:25.828452Z","iopub.execute_input":"2023-07-12T17:28:25.828828Z","iopub.status.idle":"2023-07-12T17:28:32.748144Z","shell.execute_reply.started":"2023-07-12T17:28:25.828794Z","shell.execute_reply":"2023-07-12T17:28:32.746969Z"},"trusted":true},"execution_count":55,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/110 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"260e0e759243423bb9e6d88909940410"}},"metadata":{}},{"name":"stdout","text":"Embeddings written to file successfully\nTime taken to compute and write embeddings - 6.913477182388306 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"gensim_model = load_gensim_model_from_file(\"dbpedia-ontology.vectors\")","metadata":{"execution":{"iopub.status.busy":"2023-07-12T17:28:43.145847Z","iopub.execute_input":"2023-07-12T17:28:43.146331Z","iopub.status.idle":"2023-07-12T17:28:44.364820Z","shell.execute_reply.started":"2023-07-12T17:28:43.146290Z","shell.execute_reply":"2023-07-12T17:28:44.363771Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"print(pred)\ndb_pred = ontosim_search(pred, gensim_model, encoder_model, tbox)\nprint(db_pred)","metadata":{"execution":{"iopub.status.busy":"2023-07-12T17:31:31.092434Z","iopub.execute_input":"2023-07-12T17:31:31.092808Z","iopub.status.idle":"2023-07-12T17:31:31.141227Z","shell.execute_reply.started":"2023-07-12T17:31:31.092774Z","shell.execute_reply":"2023-07-12T17:31:31.139959Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"shares border with\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a79adbe95ebd4b8283e57d35d95d359a"}},"metadata":{}},{"name":"stdout","text":"                                            URIs              label     score\n0           [http://dbpedia.org/ontology/border]             border  0.728290\n1       [http://dbpedia.org/ontology/flagBorder]        flag border  0.631572\n2  [http://dbpedia.org/ontology/hasJunctionWith]  has junction with  0.487150\n3      [http://dbpedia.org/ontology/linkedSpace]       linked space  0.423550\n4    [http://dbpedia.org/ontology/routeJunction]     route junction  0.405855\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### End-2-End","metadata":{}},{"cell_type":"code","source":"@print_durations\ndef get_triple_from_triple(sub, relation, obj, sentence):\n    \n    subject_entity = EL_GENRE(\n        annotate_sentence(sentence, sub), genre_model, genre_tokenizer)[0]\n    subject_entity = \"https://dbpedia.org/resource/\"+\"_\".join(subject_entity.split())\n    \n    object_entity = EL_GENRE(\n        annotate_sentence(sentence, obj), genre_model, genre_tokenizer)[0]\n    object_entity = \"https://dbpedia.org/resource/\"+\"_\".join(object_entity.split())\n    \n    predicates, label, score = ontosim_search(\n        relation, gensim_model, encoder_model, tbox).iloc[0].values\n    \n    predicate = None\n    for p in predicates:\n        if p.split(\"/\")[-1][0].islower():\n            predicate = p\n            break\n#     return (subject_entity, (predicate, score), object_entity)\n    return (subject_entity, predicate, object_entity)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:32:13.469689Z","iopub.execute_input":"2023-07-12T18:32:13.470088Z","iopub.status.idle":"2023-07-12T18:32:13.477853Z","shell.execute_reply.started":"2023-07-12T18:32:13.470053Z","shell.execute_reply":"2023-07-12T18:32:13.476661Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"code","source":"@print_durations\ndef get_triples_from_sentence(sentence):\n    sent_triples =  extract_relations_rebel(model=model, tokenizer=tokenizer, text=sentence)\n    triples = {}\n    \n    for i in range(len(sent_triples)):\n        subject, relation, objct, score = sent_triples.iloc[i].values\n        triple = get_triple_from_triple(subject, relation, objct, sentence)\n        triples[subject+\"_\"+relation+\"_\"+objct] = triple\n    return triples","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:30:50.825951Z","iopub.execute_input":"2023-07-12T18:30:50.826325Z","iopub.status.idle":"2023-07-12T18:30:50.835540Z","shell.execute_reply.started":"2023-07-12T18:30:50.826295Z","shell.execute_reply":"2023-07-12T18:30:50.834526Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"code","source":"triples_s0 = get_triples_from_sentence(tea_sentences[0])","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:30:57.389238Z","iopub.execute_input":"2023-07-12T18:30:57.389599Z","iopub.status.idle":"2023-07-12T18:31:34.090139Z","shell.execute_reply.started":"2023-07-12T18:30:57.389569Z","shell.execute_reply":"2023-07-12T18:31:34.089135Z"},"trusted":true},"execution_count":156,"outputs":[{"name":"stdout","text":"   36.70 s in get_triples_from_sentence('Tea is an aromatic be...)\n","output_type":"stream"}]},{"cell_type":"code","source":"for t,v in triples_s0.items():\n    print(t.replace(\"_\",\" \"))\n    print(v)\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:12:52.477404Z","iopub.execute_input":"2023-07-12T18:12:52.477820Z","iopub.status.idle":"2023-07-12T18:12:52.489462Z","shell.execute_reply.started":"2023-07-12T18:12:52.477782Z","shell.execute_reply":"2023-07-12T18:12:52.488346Z"},"trusted":true},"execution_count":121,"outputs":[{"name":"stdout","text":"China shares border with Myanmar\n('https://dbpedia.org/resource/China', 'http://dbpedia.org/ontology/border', 'https://dbpedia.org/resource/Myanmar')\n\nMyanmar shares border with China\n('https://dbpedia.org/resource/Myanmar', 'http://dbpedia.org/ontology/border', 'https://dbpedia.org/resource/China')\n\nChina part of East Asia\n('https://dbpedia.org/resource/China', 'http://dbpedia.org/ontology/part', 'https://dbpedia.org/resource/East_Asia')\n\nEast Asia has part China\n('https://dbpedia.org/resource/East_Asia', 'http://dbpedia.org/ontology/part', 'https://dbpedia.org/resource/China')\n\nMyanmar part of East Asia\n('https://dbpedia.org/resource/Myanmar', 'http://dbpedia.org/ontology/part', 'https://dbpedia.org/resource/East_Asia')\n\nMyanmar located on terrain feature East Asia\n('https://dbpedia.org/resource/Myanmar', None, 'https://dbpedia.org/resource/East_Asia')\n\nEast Asia has part Myanmar\n('https://dbpedia.org/resource/East_Asia', 'http://dbpedia.org/ontology/part', 'https://dbpedia.org/resource/Myanmar')\n\nTea subclass of beverage\n('https://dbpedia.org/resource/Tea', 'http://dbpedia.org/ontology/subClassis', 'https://dbpedia.org/resource/Drink')\n\n","output_type":"stream"}]},{"cell_type":"code","source":"triples_s200 = get_triples_from_sentence(tea_sentences[200])","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:36:18.158782Z","iopub.execute_input":"2023-07-12T18:36:18.159185Z","iopub.status.idle":"2023-07-12T18:36:58.952216Z","shell.execute_reply.started":"2023-07-12T18:36:18.159151Z","shell.execute_reply":"2023-07-12T18:36:58.950834Z"},"trusted":true},"execution_count":171,"outputs":[{"name":"stdout","text":"  487.28 ms in extract_relations_rebel(model=BartForConditionalGene..., tokenizer=BartTokenizerFast(name..., text='During the Second Wor...)\n    5.08 s in get_triple_from_triple('Canadian', 'participant in', 'Second World War', 'During the Second Wor...)\n    5.19 s in get_triple_from_triple('British', 'conflict', 'Second World War', 'During the Second Wor...)\n    4.93 s in get_triple_from_triple('British', 'participant in', 'Second World War', 'During the Second Wor...)\n    5.05 s in get_triple_from_triple('Canadian', 'conflict', 'Second World War', 'During the Second Wor...)\n    4.68 s in get_triple_from_triple('Canadian soldiers', 'conflict', 'Second World War', 'During the Second Wor...)\n    5.24 s in get_triple_from_triple('British and Canadian ..., 'conflict', 'Second World War', 'During the Second Wor...)\n    4.94 s in get_triple_from_triple('Compo', 'conflict', 'Second World War', 'During the Second Wor...)\n    5.19 s in get_triple_from_triple('composite ration pack', 'conflict', 'Second World War', 'During the Second Wor...)\n   40.79 s in get_triples_from_sentence('During the Second Wor...)\n","output_type":"stream"}]},{"cell_type":"code","source":"for t,v in triples_s200.items():\n    print(t.replace(\"_\",\" \"))\n    print(v)\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-07-12T18:37:12.976613Z","iopub.execute_input":"2023-07-12T18:37:12.976968Z","iopub.status.idle":"2023-07-12T18:37:12.983646Z","shell.execute_reply.started":"2023-07-12T18:37:12.976936Z","shell.execute_reply":"2023-07-12T18:37:12.982580Z"},"trusted":true},"execution_count":172,"outputs":[{"name":"stdout","text":"Canadian participant in Second World War\n('https://dbpedia.org/resource/Canada', 'http://dbpedia.org/ontology/participant', 'https://dbpedia.org/resource/World_War_II')\n\nBritish conflict Second World War\n('https://dbpedia.org/resource/United_Kingdom', 'http://dbpedia.org/ontology/conflict', 'https://dbpedia.org/resource/World_War_II')\n\nBritish participant in Second World War\n('https://dbpedia.org/resource/United_Kingdom', 'http://dbpedia.org/ontology/participant', 'https://dbpedia.org/resource/World_War_II')\n\nCanadian conflict Second World War\n('https://dbpedia.org/resource/Canada', 'http://dbpedia.org/ontology/conflict', 'https://dbpedia.org/resource/World_War_II')\n\nCanadian soldiers conflict Second World War\n('https://dbpedia.org/resource/Canadian_Armed_Forces', 'http://dbpedia.org/ontology/conflict', 'https://dbpedia.org/resource/World_War_II')\n\nBritish and Canadian soldiers conflict Second World War\n('https://dbpedia.org/resource/British_and_Canadian_Army_during_World_War_II', 'http://dbpedia.org/ontology/conflict', 'https://dbpedia.org/resource/World_War_II')\n\nCompo conflict Second World War\n('https://dbpedia.org/resource/Compo', 'http://dbpedia.org/ontology/conflict', 'https://dbpedia.org/resource/World_War_II')\n\ncomposite ration pack conflict Second World War\n('https://dbpedia.org/resource/Composite_ration_pack', 'http://dbpedia.org/ontology/conflict', 'https://dbpedia.org/resource/World_War_II')\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}