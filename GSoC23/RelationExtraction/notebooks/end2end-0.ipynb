{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-12T16:44:01.131470Z","iopub.status.busy":"2023-07-12T16:44:01.130809Z","iopub.status.idle":"2023-07-12T16:44:01.144213Z","shell.execute_reply":"2023-07-12T16:44:01.142912Z","shell.execute_reply.started":"2023-07-12T16:44:01.131421Z"},"trusted":true},"outputs":[],"source":["## This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":159,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:32:43.565689Z","iopub.status.busy":"2023-07-12T18:32:43.565321Z","iopub.status.idle":"2023-07-12T18:32:43.573495Z","shell.execute_reply":"2023-07-12T18:32:43.572498Z","shell.execute_reply.started":"2023-07-12T18:32:43.565657Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","from funcy import print_durations"]},{"cell_type":"markdown","metadata":{},"source":["### Fetch article"]},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2023-07-12T16:46:11.504013Z","iopub.status.busy":"2023-07-12T16:46:11.503643Z","iopub.status.idle":"2023-07-12T16:46:26.036108Z","shell.execute_reply":"2023-07-12T16:46:26.034662Z","shell.execute_reply.started":"2023-07-12T16:46:11.503957Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting wikipedia\n","  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from wikipedia) (4.12.2)\n","Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wikipedia) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.5.7)\n","Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.3.2.post1)\n","Building wheels for collected packages: wikipedia\n","  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11696 sha256=35bca2dc19216449116e0b3ce33db17a25c34cd0ef71f1ead37e849b46cc4831\n","  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n","Successfully built wikipedia\n","Installing collected packages: wikipedia\n","Successfully installed wikipedia-1.4.0\n"]}],"source":["!pip install wikipedia"]},{"cell_type":"code","execution_count":160,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:32:52.831554Z","iopub.status.busy":"2023-07-12T18:32:52.831192Z","iopub.status.idle":"2023-07-12T18:32:52.838121Z","shell.execute_reply":"2023-07-12T18:32:52.837064Z","shell.execute_reply.started":"2023-07-12T18:32:52.831524Z"},"trusted":true},"outputs":[],"source":["import wikipedia\n","\n","@print_durations\n","def get_text_of_wiki_page(article_name: str):\n","    \"\"\"Given an article name(need not be exact title of page),\n","    return the textual content of the wikipedia article.\n","    We do a search for the articles and select the top-1 result, in case\n","    where the article name is not the exact title.\n","\n","    Args:\n","        article_name (str): Name of a wikipedia article\n","\n","    Returns:\n","        str: The text of that article.\n","    \"\"\"\n","    article_name_result = wikipedia.page(wikipedia.search(article_name)[0], auto_suggest=False)\n","    article_name_content = article_name_result.content\n","    article_name_content.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n","    return article_name_content"]},{"cell_type":"code","execution_count":161,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:32:56.041239Z","iopub.status.busy":"2023-07-12T18:32:56.040121Z","iopub.status.idle":"2023-07-12T18:32:56.496355Z","shell.execute_reply":"2023-07-12T18:32:56.495250Z","shell.execute_reply.started":"2023-07-12T18:32:56.041191Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["  450.08 ms in get_text_of_wiki_page('Tea')\n"]}],"source":["tea = get_text_of_wiki_page(\"Tea\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import regex as re"]},{"cell_type":"markdown","metadata":{},"source":["### Preprocess"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T17:01:21.034728Z","iopub.status.busy":"2023-07-12T17:01:21.034324Z","iopub.status.idle":"2023-07-12T17:01:21.042413Z","shell.execute_reply":"2023-07-12T17:01:21.041165Z","shell.execute_reply.started":"2023-07-12T17:01:21.034693Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import nltk\n","nltk.download(\"punkt\")\n","from nltk import sent_tokenize\n","from nltk.tokenize import word_tokenize"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T16:48:43.522077Z","iopub.status.busy":"2023-07-12T16:48:43.521578Z","iopub.status.idle":"2023-07-12T16:48:43.574453Z","shell.execute_reply":"2023-07-12T16:48:43.573535Z","shell.execute_reply.started":"2023-07-12T16:48:43.522029Z"},"trusted":true},"outputs":[{"data":{"text/plain":["280"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["tea_sentences = sent_tokenize(tea)\n","len(tea_sentences)"]},{"cell_type":"code","execution_count":167,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:35:15.531939Z","iopub.status.busy":"2023-07-12T18:35:15.530955Z","iopub.status.idle":"2023-07-12T18:35:15.539955Z","shell.execute_reply":"2023-07-12T18:35:15.538959Z","shell.execute_reply.started":"2023-07-12T18:35:15.531899Z"},"trusted":true},"outputs":[],"source":["def annotate_sentence(sentence, mention):\n","    match = re.search(mention.lower(), sentence.lower())\n","    start, end = match.span()\n","    sentence = sentence[:start] + \" [START_ENT] \" + sentence[start:end] + \" [END_ENT] \" + sentence[end:]\n","    return sentence"]},{"cell_type":"code","execution_count":163,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:33:06.076164Z","iopub.status.busy":"2023-07-12T18:33:06.075687Z","iopub.status.idle":"2023-07-12T18:33:06.085887Z","shell.execute_reply":"2023-07-12T18:33:06.084614Z","shell.execute_reply.started":"2023-07-12T18:33:06.076121Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["   13.78 mks in annotate_sentence('Tea is an aromatic be..., 'China')\n","    9.97 mks in annotate_sentence('Tea is an aromatic be..., 'Myanmar')\n"]},{"data":{"text/plain":["('Tea is an aromatic beverage prepared by pouring hot or boiling water over cured or fresh leaves of Camellia sinensis, an evergreen shrub native to East Asia which probably originated in the borderlands of southwestern  [START_ENT] China [END_ENT]  and northern Myanmar.',\n"," 'Tea is an aromatic beverage prepared by pouring hot or boiling water over cured or fresh leaves of Camellia sinensis, an evergreen shrub native to East Asia which probably originated in the borderlands of southwestern China and northern  [START_ENT] Myanmar [END_ENT] .')"]},"execution_count":163,"metadata":{},"output_type":"execute_result"}],"source":["annotate_sentence(tea_sentences[0], \"China\"), annotate_sentence(tea_sentences[0], \"Myanmar\")"]},{"cell_type":"markdown","metadata":{},"source":["### REBEL"]},{"cell_type":"code","execution_count":149,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:27:49.731540Z","iopub.status.busy":"2023-07-12T18:27:49.730851Z","iopub.status.idle":"2023-07-12T18:28:03.794478Z","shell.execute_reply":"2023-07-12T18:28:03.793397Z","shell.execute_reply.started":"2023-07-12T18:27:49.731502Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")"]},{"cell_type":"code","execution_count":168,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:35:26.561344Z","iopub.status.busy":"2023-07-12T18:35:26.560948Z","iopub.status.idle":"2023-07-12T18:35:26.579123Z","shell.execute_reply":"2023-07-12T18:35:26.577955Z","shell.execute_reply.started":"2023-07-12T18:35:26.561312Z"},"trusted":true},"outputs":[],"source":["import ast\n","import torch\n","import pandas as pd\n","gen_kwargs = {\n","    \"max_length\": 256,\n","    \"length_penalty\": 0,\n","    \"num_beams\": 10,\n","    \"num_return_sequences\": 10,\n","}\n","\n","# Function to parse the generated text and extract the triplets\n","def extract_triplets(text):\n","    triplets = []\n","    relation, subject, relation, object_ = '', '', '', ''\n","    text = text.strip()\n","    current = 'x'\n","    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n","        if token == \"<triplet>\":\n","            current = 't'\n","            if relation != '':\n","                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n","                relation = ''\n","            subject = ''\n","        elif token == \"<subj>\":\n","            current = 's'\n","            if relation != '':\n","                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n","            object_ = ''\n","        elif token == \"<obj>\":\n","            current = 'o'\n","            relation = ''\n","        else:\n","            if current == 't':\n","                subject += ' ' + token\n","            elif current == 's':\n","                object_ += ' ' + token\n","            elif current == 'o':\n","                relation += ' ' + token\n","    if subject != '' and relation != '' and object_ != '':\n","        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n","    return triplets\n","\n","@print_durations\n","def extract_relations_rebel(model, tokenizer, text):\n","    \n","#     tokenized_sentences = sentence_tokenize(text)\n","    tokenized_sentences = [text]\n","    list_triples = []\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    for text in tokenized_sentences:\n","        model_inputs = tokenizer(text, max_length=256, padding=True, truncation=True, return_tensors = 'pt')\n","        # Generate\n","        generated_tokens = model.generate(\n","            model_inputs[\"input_ids\"].to(model.device),\n","            attention_mask=model_inputs[\"attention_mask\"].to(model.device),\n","            **gen_kwargs,\n","        )\n","\n","        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n","\n","        l1 = []\n","        for idx, sentence in enumerate(decoded_preds):\n","            #print(f'Prediction triplets sentence {idx}')\n","            #print(extract_triplets(sentence))\n","            l1 += extract_triplets(sentence)\n","\n","        d1 = {}\n","        ctr = 0\n","\n","\n","        for x in l1:\n","            ctr += 1\n","            if not str(x) in d1:\n","                d1[str(x)] = 0\n","            d1[str(x)] += 1\n","\n","        for x in d1:\n","            t = x.replace(\"}\", \"\")\n","            final_dict = t + \", 'Confidence': \" + str(d1[x]/ctr) + \"}\"\n","            #print(final_dict)\n","            final_dictionary = ast.literal_eval(final_dict)\n","            list_triples.append(final_dictionary)\n","\n","    return pd.DataFrame(list_triples).sort_values(by=\"Confidence\", ascending=False)"]},{"cell_type":"code","execution_count":173,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:44:59.763402Z","iopub.status.busy":"2023-07-12T18:44:59.763040Z","iopub.status.idle":"2023-07-12T18:44:59.772376Z","shell.execute_reply":"2023-07-12T18:44:59.771333Z","shell.execute_reply.started":"2023-07-12T18:44:59.763372Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('Tea is an aromatic beverage prepared by pouring hot or boiling water over cured or fresh leaves of Camellia sinensis, an evergreen shrub native to East Asia which probably originated in the borderlands of southwestern China and northern Myanmar.',\n"," ['Tea is an aromatic beverage prepared by pouring hot or boiling water over cured or fresh leaves of Camellia sinensis, an evergreen shrub native to East Asia which probably originated in the borderlands of southwestern China and northern Myanmar.'])"]},"execution_count":173,"metadata":{},"output_type":"execute_result"}],"source":["tea_sentences[0], sent_tokenize(tea_sentences[0])"]},{"cell_type":"code","execution_count":169,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:35:28.469968Z","iopub.status.busy":"2023-07-12T18:35:28.469608Z","iopub.status.idle":"2023-07-12T18:35:29.586863Z","shell.execute_reply":"2023-07-12T18:35:29.585844Z","shell.execute_reply.started":"2023-07-12T18:35:28.469935Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["    1.10 s in extract_relations_rebel(model=BartForConditionalGene..., tokenizer=BartTokenizerFast(name..., text='Tea is an aromatic be...)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>head</th>\n","      <th>type</th>\n","      <th>tail</th>\n","      <th>Confidence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>China</td>\n","      <td>shares border with</td>\n","      <td>Myanmar</td>\n","      <td>0.222222</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Myanmar</td>\n","      <td>shares border with</td>\n","      <td>China</td>\n","      <td>0.222222</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>China</td>\n","      <td>part of</td>\n","      <td>East Asia</td>\n","      <td>0.200000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>East Asia</td>\n","      <td>has part</td>\n","      <td>China</td>\n","      <td>0.133333</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Myanmar</td>\n","      <td>part of</td>\n","      <td>East Asia</td>\n","      <td>0.088889</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Myanmar</td>\n","      <td>located on terrain feature</td>\n","      <td>East Asia</td>\n","      <td>0.066667</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>East Asia</td>\n","      <td>has part</td>\n","      <td>Myanmar</td>\n","      <td>0.044444</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Tea</td>\n","      <td>subclass of</td>\n","      <td>beverage</td>\n","      <td>0.022222</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        head                        type       tail  Confidence\n","1      China          shares border with    Myanmar    0.222222\n","3    Myanmar          shares border with      China    0.222222\n","0      China                     part of  East Asia    0.200000\n","4  East Asia                    has part      China    0.133333\n","2    Myanmar                     part of  East Asia    0.088889\n","5    Myanmar  located on terrain feature  East Asia    0.066667\n","6  East Asia                    has part    Myanmar    0.044444\n","7        Tea                 subclass of   beverage    0.022222"]},"execution_count":169,"metadata":{},"output_type":"execute_result"}],"source":["tea_0_triples = extract_relations_rebel(model=model, tokenizer=tokenizer, text=tea_sentences[0])\n","tea_0_triples"]},{"cell_type":"markdown","metadata":{},"source":["### GENRE"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T16:59:06.990739Z","iopub.status.busy":"2023-07-12T16:59:06.990338Z","iopub.status.idle":"2023-07-12T16:59:48.920001Z","shell.execute_reply":"2023-07-12T16:59:48.919018Z","shell.execute_reply.started":"2023-07-12T16:59:06.990707Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c50e1285c6114094841d56722b326379","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c87c05e4a88434b86c95fb37db2448d","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/999k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"43777e25c7fe431c8d856a0500dacc13","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"213a17b2462347e6a19ed3384ac5f62d","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1c34eafedcc24b2b92cbddf94a33fde9","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29142db746ef4e268b6e99d799d27a75","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/997 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fbdcafed2855492c9d0a2bcd4159a529","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1004ca5112044bc92fa602f6f1ae026","version_major":2,"version_minor":0},"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/196 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","genre_tokenizer = AutoTokenizer.from_pretrained(\"facebook/genre-linking-blink\")\n","genre_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/genre-linking-blink\").eval()"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T17:08:38.072913Z","iopub.status.busy":"2023-07-12T17:08:38.071967Z","iopub.status.idle":"2023-07-12T17:08:38.080399Z","shell.execute_reply":"2023-07-12T17:08:38.079282Z","shell.execute_reply.started":"2023-07-12T17:08:38.072865Z"},"trusted":true},"outputs":[],"source":["def EL_GENRE(annotated_sentences, model, tokenizer):\n","    \"\"\"A method to perform entity linking for entity-mentions annotated\n","    in sentences using the GENRE model.\n","\n","    ```\n","    tokenizer = AutoTokenizer.from_pretrained(\"facebook/genre-linking-blink\")\n","    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/genre-linking-blink\").eval()\n","\n","    sentences = [\n","        \"[START_ENT] England [END_ENT] won the cricket world cup in 2019\",\n","        \"I just finished reading [START_ENT] 'The Jungle Book' [END_ENT]\",\n","        \"India is a country in Asia. [START_ENT] It [END_ENT] has a rich cultural heritage\"\n","    ]\n","\n","    EL_GENRE(annotated_sentences=sentences, model=model, tokenizer=tokenizer)\n","    \n","    ```\n","\n","    Args:\n","        annotated_sentences (list): A list of sentences annotated with entity-mentions\n","        model : GENRE model from huggingface hub\n","        tokenizer : Appropriate tokenizer for GENRE model\n","    \"\"\"\n","    outputs = model.generate(\n","    **tokenizer(annotated_sentences, return_tensors=\"pt\", padding=True),\n","    num_beams=5,\n","    num_return_sequences=1,\n","    # OPTIONAL: use constrained beam search\n","    # prefix_allowed_tokens_fn=lambda batch_id, sent: trie.get(sent.tolist()),\n","    )\n","\n","    entites = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","    # These entites are in the form of wikipedia page titles. Need to \n","    # add the https://dbpedia/resource to each of them as postprocessing step\n","    return entites"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T17:11:06.284114Z","iopub.status.busy":"2023-07-12T17:11:06.283726Z","iopub.status.idle":"2023-07-12T17:11:06.289170Z","shell.execute_reply":"2023-07-12T17:11:06.288203Z","shell.execute_reply.started":"2023-07-12T17:11:06.284080Z"},"trusted":true},"outputs":[],"source":["sub, pred, obj, score = tea_0_triples.iloc[0].values"]},{"cell_type":"code","execution_count":150,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:28:14.878888Z","iopub.status.busy":"2023-07-12T18:28:14.878524Z","iopub.status.idle":"2023-07-12T18:28:18.760966Z","shell.execute_reply":"2023-07-12T18:28:18.759926Z","shell.execute_reply.started":"2023-07-12T18:28:14.878856Z"},"trusted":true},"outputs":[],"source":["sub_ent = EL_GENRE(annotate_sentence(tea_sentences[0], sub), genre_model, genre_tokenizer)\n","obj_ent = EL_GENRE(annotate_sentence(tea_sentences[0], obj), genre_model, genre_tokenizer)"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T17:12:44.856877Z","iopub.status.busy":"2023-07-12T17:12:44.856518Z","iopub.status.idle":"2023-07-12T17:12:44.864277Z","shell.execute_reply":"2023-07-12T17:12:44.863317Z","shell.execute_reply.started":"2023-07-12T17:12:44.856847Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(['China'], ['Myanmar'])"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["sub_ent, obj_ent"]},{"cell_type":"markdown","metadata":{},"source":["### Onto-Embeddings"]},{"cell_type":"code","execution_count":45,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2023-07-12T17:21:59.315272Z","iopub.status.busy":"2023-07-12T17:21:59.314686Z","iopub.status.idle":"2023-07-12T17:22:15.536823Z","shell.execute_reply":"2023-07-12T17:22:15.535446Z","shell.execute_reply.started":"2023-07-12T17:21:59.315237Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Collecting sentence-transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (4.3.1)\n","Collecting SPARQLWrapper\n","  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.30.2)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.65.0)\n","Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.0.0)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.15.1)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.23.5)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.1)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (3.2.4)\n","Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\n","Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.16.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (6.3.0)\n","Collecting rdflib>=6.1.1 (from SPARQLWrapper)\n","  Downloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.6.3)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n","Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=6.1.1->SPARQLWrapper)\n","  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyparsing<4,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.0.9)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.1)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.16.0)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.5.7)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=15d96ab15126a91fae9979d9f7ff3ed6a5cddee440b4d75bdf9b183bbc03d91c\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n","Successfully built sentence-transformers\n","Installing collected packages: isodate, rdflib, SPARQLWrapper, sentence-transformers\n","Successfully installed SPARQLWrapper-2.0.0 isodate-0.6.1 rdflib-6.3.2 sentence-transformers-2.2.2\n"]}],"source":["!pip3 install sentence-transformers gensim SPARQLWrapper"]},{"cell_type":"code","execution_count":170,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:35:52.025421Z","iopub.status.busy":"2023-07-12T18:35:52.025021Z","iopub.status.idle":"2023-07-12T18:35:52.368065Z","shell.execute_reply":"2023-07-12T18:35:52.367053Z","shell.execute_reply.started":"2023-07-12T18:35:52.025391Z"},"trusted":true},"outputs":[],"source":["from sentence_transformers import SentenceTransformer\n","encoder_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n","\n","def get_sentence_transformer_model(model_name):\n","    model = SentenceTransformer(model_name_or_path=model_name)\n","    return model\n","\n","@print_durations\n","def get_embeddings(labels, sent_tran_model):\n","    embeddings = sent_tran_model.encode(labels, show_progress_bar=False)\n","    return embeddings"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T17:28:14.012215Z","iopub.status.busy":"2023-07-12T17:28:14.011805Z","iopub.status.idle":"2023-07-12T17:28:14.025769Z","shell.execute_reply":"2023-07-12T17:28:14.024632Z","shell.execute_reply.started":"2023-07-12T17:28:14.012180Z"},"trusted":true},"outputs":[],"source":["from SPARQLWrapper import SPARQLWrapper, JSON\n","\n","NS_RESOURCE = 'http://dbpedia.org/resource/'\n","NS_RESOURCE_LEN = len(NS_RESOURCE)\n","\n","NS_ONTOLOGY = 'http://dbpedia.org/ontology/'\n","NS_ONTOLOGY_LEN = len(NS_ONTOLOGY)\n","\n","\n","def retrieve_tbox(lang='en', offset=0):\n","    sparql = SPARQLWrapper('http://dbpedia.org/sparql')\n","    query = f\"\"\"\n","    SELECT ?uri ?label {{\n","      ?uri a ?type ; rdfs:label ?label .\n","      values(?type) {{ (owl:Class) (rdf:Property) }}\n","      filter(lang(?label) = '{lang}' && regex(?uri, \"http://dbpedia.org/ontology/\"))\n","    }} LIMIT 10000 OFFSET {offset}\n","    \"\"\"\n","    sparql.setQuery(query)\n","    sparql.setReturnFormat(JSON)\n","    results = sparql.query().convert()\n","    \n","    tbox = {}\n","    for result in results['results']['bindings']:\n","        uri = result['uri']['value']\n","        label = result['label']['value']\n","        if label not in tbox:\n","            tbox[label] = set()\n","        tbox[label].add(uri)\n","    return tbox\n","\n","def get_labels_and_tbox():\n","    offset = 0\n","    tbox = {}\n","    while True:\n","        tbox_chunk = retrieve_tbox(lang='en', offset=offset)\n","        if len(tbox_chunk) == 0:\n","            break\n","        offset += 10000\n","        for k, v in tbox_chunk.items():\n","            if k not in tbox:\n","                tbox[k] = set()\n","            tbox[k] = tbox[k].union(v)\n","    labels = [l.replace('\\n', ' ') for l in tbox]\n","    return labels, tbox\n","\n","def to_uri(label, tbox):\n","    return list(filter(lambda x: 'A' <= x[NS_ONTOLOGY_LEN : NS_ONTOLOGY_LEN+1] <= 'z', tbox[label]))\n","\n","def write_embeddings_to_file(embeddings, labels, filename):\n","    with open(filename, 'w', encoding='utf-8') as f_out:\n","        f_out.write(f\"{len(labels)} {len(embeddings[0])}\\n\")\n","        for label, embedding in zip(labels, embeddings):\n","            f_out.write(f\"{label.replace(' ', '_')} {' '.join([str(x) for x in embedding])}\\n\")\n","    print(\"Embeddings written to file successfully\")"]},{"cell_type":"code","execution_count":147,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:26:32.261332Z","iopub.status.busy":"2023-07-12T18:26:32.260925Z","iopub.status.idle":"2023-07-12T18:26:32.269192Z","shell.execute_reply":"2023-07-12T18:26:32.268062Z","shell.execute_reply.started":"2023-07-12T18:26:32.261297Z"},"trusted":true},"outputs":[],"source":["from gensim.models import KeyedVectors\n","\n","\n","def ontosim_search(term, gensim_model, sent_tran_model, tbox):\n","    result = gensim_model.most_similar(\n","        positive=sent_tran_model.encode([term], show_progress_bar=False), topn=5)\n","    out = []\n","    for label, score in result:\n","        out.append({'label': label.replace('_', ' '), 'score': score})\n","    df = pd.DataFrame(out)\n","    df.insert(0, 'URIs', df['label'].map(lambda x: to_uri(x, tbox=tbox)))\n","    return df\n","\n","def load_gensim_model_from_file(filepath):\n","    model = KeyedVectors.load_word2vec_format(filepath, binary=False)\n","    return model"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T17:28:25.828828Z","iopub.status.busy":"2023-07-12T17:28:25.828452Z","iopub.status.idle":"2023-07-12T17:28:32.748144Z","shell.execute_reply":"2023-07-12T17:28:32.746969Z","shell.execute_reply.started":"2023-07-12T17:28:25.828794Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"260e0e759243423bb9e6d88909940410","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/110 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Embeddings written to file successfully\n","Time taken to compute and write embeddings - 6.913477182388306 seconds\n"]}],"source":["import time\n","start = time.time()\n","labels, tbox = get_labels_and_tbox()\n","embeddings = get_embeddings(labels, encoder_model)\n","write_embeddings_to_file(embeddings, labels, \"dbpedia-ontology.vectors\")\n","end = time.time()\n","print(f\"Time taken to compute and write embeddings - {end-start} seconds\")"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T17:28:43.146331Z","iopub.status.busy":"2023-07-12T17:28:43.145847Z","iopub.status.idle":"2023-07-12T17:28:44.364820Z","shell.execute_reply":"2023-07-12T17:28:44.363771Z","shell.execute_reply.started":"2023-07-12T17:28:43.146290Z"},"trusted":true},"outputs":[],"source":["gensim_model = load_gensim_model_from_file(\"dbpedia-ontology.vectors\")"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T17:31:31.092808Z","iopub.status.busy":"2023-07-12T17:31:31.092434Z","iopub.status.idle":"2023-07-12T17:31:31.141227Z","shell.execute_reply":"2023-07-12T17:31:31.139959Z","shell.execute_reply.started":"2023-07-12T17:31:31.092774Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["shares border with\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a79adbe95ebd4b8283e57d35d95d359a","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["                                            URIs              label     score\n","0           [http://dbpedia.org/ontology/border]             border  0.728290\n","1       [http://dbpedia.org/ontology/flagBorder]        flag border  0.631572\n","2  [http://dbpedia.org/ontology/hasJunctionWith]  has junction with  0.487150\n","3      [http://dbpedia.org/ontology/linkedSpace]       linked space  0.423550\n","4    [http://dbpedia.org/ontology/routeJunction]     route junction  0.405855\n"]}],"source":["print(pred)\n","db_pred = ontosim_search(pred, gensim_model, encoder_model, tbox)\n","print(db_pred)"]},{"cell_type":"markdown","metadata":{},"source":["### End-2-End"]},{"cell_type":"code","execution_count":158,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:32:13.470088Z","iopub.status.busy":"2023-07-12T18:32:13.469689Z","iopub.status.idle":"2023-07-12T18:32:13.477853Z","shell.execute_reply":"2023-07-12T18:32:13.476661Z","shell.execute_reply.started":"2023-07-12T18:32:13.470053Z"},"trusted":true},"outputs":[],"source":["@print_durations\n","def get_triple_from_triple(sub, relation, obj, sentence):\n","    \n","    subject_entity = EL_GENRE(\n","        annotate_sentence(sentence, sub), genre_model, genre_tokenizer)[0]\n","    subject_entity = \"https://dbpedia.org/resource/\"+\"_\".join(subject_entity.split())\n","    \n","    object_entity = EL_GENRE(\n","        annotate_sentence(sentence, obj), genre_model, genre_tokenizer)[0]\n","    object_entity = \"https://dbpedia.org/resource/\"+\"_\".join(object_entity.split())\n","    \n","    predicates, label, score = ontosim_search(\n","        relation, gensim_model, encoder_model, tbox).iloc[0].values\n","    \n","    predicate = None\n","    for p in predicates:\n","        if p.split(\"/\")[-1][0].islower():\n","            predicate = p\n","            break\n","#     return (subject_entity, (predicate, score), object_entity)\n","    return (subject_entity, predicate, object_entity)\n","    "]},{"cell_type":"code","execution_count":154,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:30:50.826325Z","iopub.status.busy":"2023-07-12T18:30:50.825951Z","iopub.status.idle":"2023-07-12T18:30:50.835540Z","shell.execute_reply":"2023-07-12T18:30:50.834526Z","shell.execute_reply.started":"2023-07-12T18:30:50.826295Z"},"trusted":true},"outputs":[],"source":["@print_durations\n","def get_triples_from_sentence(sentence):\n","    sent_triples =  extract_relations_rebel(model=model, tokenizer=tokenizer, text=sentence)\n","    triples = {}\n","    \n","    for i in range(len(sent_triples)):\n","        subject, relation, objct, score = sent_triples.iloc[i].values\n","        triple = get_triple_from_triple(subject, relation, objct, sentence)\n","        triples[subject+\"_\"+relation+\"_\"+objct] = triple\n","    return triples"]},{"cell_type":"code","execution_count":156,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:30:57.389599Z","iopub.status.busy":"2023-07-12T18:30:57.389238Z","iopub.status.idle":"2023-07-12T18:31:34.090139Z","shell.execute_reply":"2023-07-12T18:31:34.089135Z","shell.execute_reply.started":"2023-07-12T18:30:57.389569Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["   36.70 s in get_triples_from_sentence('Tea is an aromatic be...)\n"]}],"source":["triples_s0 = get_triples_from_sentence(tea_sentences[0])"]},{"cell_type":"code","execution_count":121,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:12:52.477820Z","iopub.status.busy":"2023-07-12T18:12:52.477404Z","iopub.status.idle":"2023-07-12T18:12:52.489462Z","shell.execute_reply":"2023-07-12T18:12:52.488346Z","shell.execute_reply.started":"2023-07-12T18:12:52.477782Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["China shares border with Myanmar\n","('https://dbpedia.org/resource/China', 'http://dbpedia.org/ontology/border', 'https://dbpedia.org/resource/Myanmar')\n","\n","Myanmar shares border with China\n","('https://dbpedia.org/resource/Myanmar', 'http://dbpedia.org/ontology/border', 'https://dbpedia.org/resource/China')\n","\n","China part of East Asia\n","('https://dbpedia.org/resource/China', 'http://dbpedia.org/ontology/part', 'https://dbpedia.org/resource/East_Asia')\n","\n","East Asia has part China\n","('https://dbpedia.org/resource/East_Asia', 'http://dbpedia.org/ontology/part', 'https://dbpedia.org/resource/China')\n","\n","Myanmar part of East Asia\n","('https://dbpedia.org/resource/Myanmar', 'http://dbpedia.org/ontology/part', 'https://dbpedia.org/resource/East_Asia')\n","\n","Myanmar located on terrain feature East Asia\n","('https://dbpedia.org/resource/Myanmar', None, 'https://dbpedia.org/resource/East_Asia')\n","\n","East Asia has part Myanmar\n","('https://dbpedia.org/resource/East_Asia', 'http://dbpedia.org/ontology/part', 'https://dbpedia.org/resource/Myanmar')\n","\n","Tea subclass of beverage\n","('https://dbpedia.org/resource/Tea', 'http://dbpedia.org/ontology/subClassis', 'https://dbpedia.org/resource/Drink')\n","\n"]}],"source":["for t,v in triples_s0.items():\n","    print(t.replace(\"_\",\" \"))\n","    print(v)\n","    print()"]},{"cell_type":"code","execution_count":171,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:36:18.159185Z","iopub.status.busy":"2023-07-12T18:36:18.158782Z","iopub.status.idle":"2023-07-12T18:36:58.952216Z","shell.execute_reply":"2023-07-12T18:36:58.950834Z","shell.execute_reply.started":"2023-07-12T18:36:18.159151Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["  487.28 ms in extract_relations_rebel(model=BartForConditionalGene..., tokenizer=BartTokenizerFast(name..., text='During the Second Wor...)\n","    5.08 s in get_triple_from_triple('Canadian', 'participant in', 'Second World War', 'During the Second Wor...)\n","    5.19 s in get_triple_from_triple('British', 'conflict', 'Second World War', 'During the Second Wor...)\n","    4.93 s in get_triple_from_triple('British', 'participant in', 'Second World War', 'During the Second Wor...)\n","    5.05 s in get_triple_from_triple('Canadian', 'conflict', 'Second World War', 'During the Second Wor...)\n","    4.68 s in get_triple_from_triple('Canadian soldiers', 'conflict', 'Second World War', 'During the Second Wor...)\n","    5.24 s in get_triple_from_triple('British and Canadian ..., 'conflict', 'Second World War', 'During the Second Wor...)\n","    4.94 s in get_triple_from_triple('Compo', 'conflict', 'Second World War', 'During the Second Wor...)\n","    5.19 s in get_triple_from_triple('composite ration pack', 'conflict', 'Second World War', 'During the Second Wor...)\n","   40.79 s in get_triples_from_sentence('During the Second Wor...)\n"]}],"source":["triples_s200 = get_triples_from_sentence(tea_sentences[200])"]},{"cell_type":"code","execution_count":172,"metadata":{"execution":{"iopub.execute_input":"2023-07-12T18:37:12.976968Z","iopub.status.busy":"2023-07-12T18:37:12.976613Z","iopub.status.idle":"2023-07-12T18:37:12.983646Z","shell.execute_reply":"2023-07-12T18:37:12.982580Z","shell.execute_reply.started":"2023-07-12T18:37:12.976936Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Canadian participant in Second World War\n","('https://dbpedia.org/resource/Canada', 'http://dbpedia.org/ontology/participant', 'https://dbpedia.org/resource/World_War_II')\n","\n","British conflict Second World War\n","('https://dbpedia.org/resource/United_Kingdom', 'http://dbpedia.org/ontology/conflict', 'https://dbpedia.org/resource/World_War_II')\n","\n","British participant in Second World War\n","('https://dbpedia.org/resource/United_Kingdom', 'http://dbpedia.org/ontology/participant', 'https://dbpedia.org/resource/World_War_II')\n","\n","Canadian conflict Second World War\n","('https://dbpedia.org/resource/Canada', 'http://dbpedia.org/ontology/conflict', 'https://dbpedia.org/resource/World_War_II')\n","\n","Canadian soldiers conflict Second World War\n","('https://dbpedia.org/resource/Canadian_Armed_Forces', 'http://dbpedia.org/ontology/conflict', 'https://dbpedia.org/resource/World_War_II')\n","\n","British and Canadian soldiers conflict Second World War\n","('https://dbpedia.org/resource/British_and_Canadian_Army_during_World_War_II', 'http://dbpedia.org/ontology/conflict', 'https://dbpedia.org/resource/World_War_II')\n","\n","Compo conflict Second World War\n","('https://dbpedia.org/resource/Compo', 'http://dbpedia.org/ontology/conflict', 'https://dbpedia.org/resource/World_War_II')\n","\n","composite ration pack conflict Second World War\n","('https://dbpedia.org/resource/Composite_ration_pack', 'http://dbpedia.org/ontology/conflict', 'https://dbpedia.org/resource/World_War_II')\n","\n"]}],"source":["for t,v in triples_s200.items():\n","    print(t.replace(\"_\",\" \"))\n","    print(v)\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":4}
