{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f57f57d-55c9-48b6-b714-7d67fc901caf",
   "metadata": {},
   "source": [
    "# 01 Seed Preparation\n",
    "\n",
    "This code aims to extract the cause-effect pairs and corresponding sentences from 'SemEval2010_task8', to prepare them to train in the following steps.   \n",
    "\n",
    "* **Input**: The tagged text from [this link of SemEval2010_task8](https://docs.google.com/leaf?id=0B_jQiLugGTAkMDQ5ZjZiMTUtMzQ1Yy00YWNmLWJlZDYtOWY1ZDMwY2U4YjFk&sort=name&layout=list&num=50)\n",
    "* **Approaches**: Using the linguistic patterns to extract the tagged cause-effect pairs\n",
    "* **Output**: The causal pairs ans corresponding sentences stored in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afd7851e-ee79-49bf-83c5-758a538688e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import necessary packages\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0605b525-242c-4b88-938e-038bede50818",
   "metadata": {},
   "source": [
    "Please download the source data for seed pairs, by executing the file **download_data.sh** in the path of this folder\n",
    "> sh download_data.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31773888-a33b-4c11-bbf2-204836039296",
   "metadata": {},
   "outputs": [],
   "source": [
    "### define the path of dataset\n",
    "\n",
    "path_here = os.getcwd()\n",
    "path_semEval2010 = path_here +'/data/SemEval2010_task8_all_data/'\n",
    "# training pairs\n",
    "path_semEval2010_train = path_semEval2010 +'SemEval2010_task8_training/TRAIN_FILE.TXT'\n",
    "# test pairs\n",
    "path_semEval2010_test = path_semEval2010 +'SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d42106-4419-49ab-9a6e-fc57cb3852c0",
   "metadata": {},
   "source": [
    "## 01-A. Get the causal positive pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf08a60-fd8c-462c-99d5-8d658e9db1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### purpose: get the causality sentences/pairs and other sentences/pairs from the tagged text and write them into dataframe\n",
    "### input: the path of tagged text\n",
    "### output: the dataframe to store causal pairs DataFrame(columns=['SentID', 'Cause','Effect','Label', 'Sent'])\n",
    "\n",
    "def getSentSemEval(path_semEval2010): \n",
    "\n",
    "    # read the dataset\n",
    "    with open(path_semEval2010, 'r') as f:\n",
    "        content = f.readlines()\n",
    "        content = [x.strip() for x in content] \n",
    "\n",
    "\n",
    "    #------------- get the causal pairs and negative pairs in dataset -----------#\n",
    "    pattern_causal = 'Cause-Effect\\((e.),(e.)\\)'\n",
    "    pattern_other_ref = '^\\S*-\\S*\\((e.),(e.)\\)$'\n",
    "    pattern_e1 = '.*<e1>(.*)</e1>.*'        \n",
    "    pattern_e2 = '.*<e2>(.*)</e2>.*'\n",
    "    pattern_sentID = '(\\d+)\\\\t.*'\n",
    "\n",
    "    sent_list = []\n",
    "    sent_id_list = []\n",
    "    pair_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for inx_l, lines in enumerate(content):\n",
    "\n",
    "        # sentence content   \n",
    "        res1 = re.match('\\d*\\\\t\\\"(.*)\\\"', lines)\n",
    "        if res1 is not None: \n",
    "            #get rid of other noise symbols of this sentence\n",
    "            sent = re.match('\\d*\\\\t\\\"(.*)\\\"', lines)[1]\n",
    "            #delete the tags <e > in sentences\n",
    "            sent = re.sub(\"</?e[1-2]>\", \"\", sent)  \n",
    "            sent_list.append(sent)\n",
    "\n",
    "            #sentence id\n",
    "            sent_id = re.match(\"(\\d.*)\\\\t\", lines)[1]\n",
    "            sent_id_list.append(int(sent_id))\n",
    "\n",
    "            # extract entities\n",
    "            res_e1 = re.match(pattern_e1, lines)[1]\n",
    "            res_e2 = re.match(pattern_e2, lines)[1]\n",
    "\n",
    "            ### the next line\n",
    "            # causal pairs\n",
    "            res2 = re.match(pattern_causal, content[inx_l+1]) \n",
    "            if res2 is not None:\n",
    "                # cause part + effect part (e1 or e2)\n",
    "                if res2[1] == 'e1':\n",
    "                    res_cause = res_e1\n",
    "                    res_effect = res_e2\n",
    "                if res2[1] == 'e2':\n",
    "                    res_cause = res_e2\n",
    "                    res_effect = res_e1\n",
    "                # append the e1 then e2\n",
    "                pair_list.append([res_cause.lower(), res_effect.lower()])\n",
    "                label_list.append(1)\n",
    "            else:\n",
    "                pair_list.append([res_e1.lower(), res_e2.lower()])\n",
    "                label_list.append(0)\n",
    "\n",
    "\n",
    "    ### put all into df\n",
    "    df_pairs = pd.DataFrame(columns=['SentID', 'Cause','Effect','Label', 'Sent'])\n",
    "    df_pairs['SentID'] = sent_id_list\n",
    "    df_pairs['Cause'] = [i[0] for i in pair_list]\n",
    "    df_pairs['Effect'] = [i[1] for i in pair_list]\n",
    "    df_pairs['Label'] = label_list\n",
    "    df_pairs['Sent'] = sent_list\n",
    "\n",
    "    # firstly extract postive rows\n",
    "    df_pairs_p = df_pairs[df_pairs['Label'] == 1]\n",
    "    # secondly extract negative rows\n",
    "    df_pairs_n = df_pairs[df_pairs['Label'] == 0].sample(n = len(df_pairs_p))\n",
    "    # converge the two df together\n",
    "    df_pairsPN = pd.concat([df_pairs_p, df_pairs_n])\n",
    "    \n",
    "    return df_pairsPN\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e1d924b-ab7b-48d6-bb3d-5e1144fb564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the casual sentences/pairs from train files and test files of semEval2010\n",
    "df_train_semEval = getSentSemEval(path_semEval2010_train)\n",
    "df_test_semEval = getSentSemEval(path_semEval2010_test)\n",
    "\n",
    "# save to \n",
    "df_train_semEval.to_csv(path_here + '/res/df_train_semEval.csv')\n",
    "df_train_semEval.to_pickle(path_here + '/res/df_train_semEval.pkl')\n",
    "df_test_semEval.to_csv(path_here + '/res/df_test_semEval.csv')\n",
    "df_test_semEval.to_pickle(path_here + '/res/df_test_semEval.pkl')\n",
    "\n",
    "\n",
    "# get the seed pairs in train set\n",
    "\n",
    "causality_pairs_list = []\n",
    "for inx_r, row in df_train_semEval[df_train_semEval['Label']==1].iterrows():\n",
    "    causality_pairs_list.append([row['Cause'], row['Effect']])\n",
    "\n",
    "with open(path_here+'/res/causality_pairs_list.pickle', 'wb') as f:\n",
    "    pickle.dump(causality_pairs_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd9cf475-2db7-4b45-b7f0-b7ab603c6524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentID</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Label</th>\n",
       "      <th>Sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>infection</td>\n",
       "      <td>inflammation</td>\n",
       "      <td>1</td>\n",
       "      <td>The current view is that the chronic inflammat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>pressure</td>\n",
       "      <td>burst</td>\n",
       "      <td>1</td>\n",
       "      <td>The burst has been caused by water hammer pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>singer</td>\n",
       "      <td>commotion</td>\n",
       "      <td>1</td>\n",
       "      <td>The singer, who performed three of the nominat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>suicide</td>\n",
       "      <td>death</td>\n",
       "      <td>1</td>\n",
       "      <td>Suicide is one of the leading causes of death ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>mold</td>\n",
       "      <td>headaches</td>\n",
       "      <td>1</td>\n",
       "      <td>He had chest pains and headaches from mold in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>2587</td>\n",
       "      <td>band</td>\n",
       "      <td>epic</td>\n",
       "      <td>0</td>\n",
       "      <td>Returning with their new single 'We Want War' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3358</th>\n",
       "      <td>3359</td>\n",
       "      <td>trucks</td>\n",
       "      <td>parts</td>\n",
       "      <td>0</td>\n",
       "      <td>The trucks are comprised of smaller parts, inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6722</th>\n",
       "      <td>6723</td>\n",
       "      <td>plot</td>\n",
       "      <td>crises</td>\n",
       "      <td>0</td>\n",
       "      <td>The melodramatic plot dealt with crises of hum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>1067</td>\n",
       "      <td>train</td>\n",
       "      <td>doors</td>\n",
       "      <td>0</td>\n",
       "      <td>This train has as many as six sets of doors on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2649</th>\n",
       "      <td>2650</td>\n",
       "      <td>phone</td>\n",
       "      <td>washer</td>\n",
       "      <td>0</td>\n",
       "      <td>The phone went into the washer.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2006 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SentID      Cause        Effect  Label  \\\n",
       "6          7  infection  inflammation      1   \n",
       "13        14   pressure         burst      1   \n",
       "22        23     singer     commotion      1   \n",
       "26        27    suicide         death      1   \n",
       "31        32       mold     headaches      1   \n",
       "...      ...        ...           ...    ...   \n",
       "2586    2587       band          epic      0   \n",
       "3358    3359     trucks         parts      0   \n",
       "6722    6723       plot        crises      0   \n",
       "1066    1067      train         doors      0   \n",
       "2649    2650      phone        washer      0   \n",
       "\n",
       "                                                   Sent  \n",
       "6     The current view is that the chronic inflammat...  \n",
       "13    The burst has been caused by water hammer pres...  \n",
       "22    The singer, who performed three of the nominat...  \n",
       "26    Suicide is one of the leading causes of death ...  \n",
       "31    He had chest pains and headaches from mold in ...  \n",
       "...                                                 ...  \n",
       "2586  Returning with their new single 'We Want War' ...  \n",
       "3358  The trucks are comprised of smaller parts, inc...  \n",
       "6722  The melodramatic plot dealt with crises of hum...  \n",
       "1066  This train has as many as six sets of doors on...  \n",
       "2649                    The phone went into the washer.  \n",
       "\n",
       "[2006 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_semEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c4fc71-b0b9-4e2c-94f8-827a9251fd74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
