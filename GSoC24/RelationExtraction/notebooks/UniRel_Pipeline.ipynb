{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Unirel Entire pipeline in a notebook https://github.com/wtangdev/UniRel"
      ],
      "metadata": {
        "id": "SbqyI73SXGni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbF7Cko8PSrP",
        "outputId": "c03f3ecb-10e5-4a60-88ab-8be5c044c392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.45.0.dev0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGQiH5H3U1X9"
      },
      "outputs": [],
      "source": [
        "nyt_rel2text = {\n",
        "                '/business/company/advisors': 'advisors',\n",
        "                '/business/company/founders': 'founders',\n",
        "                '/business/company/industry': 'industry',\n",
        "                '/business/company/major_shareholders': 'holding',\n",
        "                '/business/company/place_founded': 'founded',\n",
        "                '/business/company_shareholder/major_shareholder_of':\n",
        "                'shareholder',\n",
        "                '/business/person/company': 'company',\n",
        "                '/location/administrative_division/country': 'country',\n",
        "                '/location/country/administrative_divisions': 'administrative',\n",
        "                '/location/country/capital': 'capital',\n",
        "                '/location/location/contains': 'contains',\n",
        "                '/location/neighborhood/neighborhood_of': 'neighbor',\n",
        "                '/people/deceased_person/place_of_death': 'death',\n",
        "                '/people/ethnicity/geographic_distribution': 'geographic',\n",
        "                '/people/ethnicity/people': 'people',\n",
        "                '/people/person/children': 'children',\n",
        "                '/people/person/ethnicity': 'ethnicity',\n",
        "                '/people/person/nationality': 'nationality',\n",
        "                '/people/person/place_lived': 'lived',\n",
        "                '/people/person/place_of_birth': 'birthplace',\n",
        "                '/people/person/profession': 'profession',\n",
        "                '/people/person/religion': 'religion',\n",
        "                '/sports/sports_team/location': 'location',\n",
        "                '/sports/sports_team_location/teams': 'teams'\n",
        "            }\n",
        "\n",
        "nyt_rel2text_ablation = {\n",
        "                # '/business/company/advisors': 'adviser',\n",
        "                '/business/company/advisors': 'counselor',\n",
        "                '/business/company/founders': 'creator',\n",
        "                '/business/company/industry': 'sector',\n",
        "                '/business/company/major_shareholders': 'shareholder',\n",
        "                '/business/company/place_founded': 'establish',\n",
        "                '/business/company_shareholder/major_shareholder_of':\n",
        "                'holder',\n",
        "                '/business/person/company': 'corporation',\n",
        "                '/location/administrative_division/country': 'state',\n",
        "                '/location/country/administrative_divisions': 'administration',\n",
        "                '/location/country/capital': 'Capital',\n",
        "                '/location/location/contains': 'include',\n",
        "                '/location/neighborhood/neighborhood_of': 'neighbour',\n",
        "                '/people/deceased_person/place_of_death': 'Death',\n",
        "                '/people/ethnicity/geographic_distribution': 'Geographic',\n",
        "                '/people/ethnicity/people': 'People',\n",
        "                '/people/person/children': 'Children',\n",
        "                '/people/person/ethnicity': 'ethnic',\n",
        "                '/people/person/nationality': 'national',\n",
        "                '/people/person/place_lived': 'live',\n",
        "                '/people/person/place_of_birth': 'birth',\n",
        "                '/people/person/profession': 'career',\n",
        "                '/people/person/religion': 'Religion',\n",
        "                '/sports/sports_team/location': 'Location',\n",
        "                '/sports/sports_team_location/teams': 'Teams'\n",
        "            }\n",
        "\n",
        "webnlg_rel2text = {\n",
        "    \"1st_runway_LengthFeet\": 1,\n",
        "    \"1st_runway_LengthMetre\": 2,\n",
        "    \"1st_runway_Number\": 3,\n",
        "    \"1st_runway_SurfaceType\": 4,\n",
        "    \"2nd_runway_SurfaceType\": 5,\n",
        "    \"3rd_runway_LengthFeet\": 6,\n",
        "    \"3rd_runway_SurfaceType\": 7,\n",
        "    \"4th_runway_LengthFeet\": 8,\n",
        "    \"5th_runway_Number\": 9,\n",
        "    \"CODEN_code\": 10,\n",
        "    \"EISSN_number\": 11,\n",
        "    \"IATA_Location_Identifier\": \"IATA\",\n",
        "    \"ICAO_Location_Identifier\": \"ICAO\",\n",
        "    \"ISBN_number\": \"ISBN\",\n",
        "    \"ISSN_number\": \"ISSN\",\n",
        "    \"LCCN_number\": 12,\n",
        "    \"LibraryofCongressClassification\": 13,\n",
        "    \"OCLC_number\": \"OCLC\",\n",
        "    \"ReferenceNumber in the National Register of Historic Places\": 14,\n",
        "    \"abbreviation\": \"abbreviation\",\n",
        "    \"academicDiscipline\": \"discipline\",\n",
        "    \"academicStaffSize\": \"staff\",\n",
        "    \"added to the National Register of Historic Places\": \"historic\",\n",
        "    \"address\": \"address\",\n",
        "    \"administrativeArrondissement\": \"arrondissement\",\n",
        "    \"administrativeCounty\": \"administrative\",\n",
        "    \"affiliation\": \"affiliation\",\n",
        "    \"affiliations\": \"affiliate\",\n",
        "    \"aircraftFighter\": \"fighter\",\n",
        "    \"aircraftHelicopter\": \"helicopter\",\n",
        "    \"almaMater\": \"alma\",\n",
        "    \"alternativeName\": \"alternative\",\n",
        "    \"anthem\": \"anthem\",\n",
        "    \"architect\": \"architect\",\n",
        "    \"architecturalStyle\": \"style\",\n",
        "    \"architecture\": \"architecture\",\n",
        "    \"areaCode\": \"area\",\n",
        "    \"areaOfLand\": \"land\",\n",
        "    \"areaOfWater\": \"water\",\n",
        "    \"areaTotal\": \"areas\",\n",
        "    \"attackAircraft\": \"attack\",\n",
        "    \"author\": \"author\",\n",
        "    \"award\": \"award\",\n",
        "    \"awards\": \"awards\",\n",
        "    \"backup pilot\": \"pilot\",\n",
        "    \"battles\": \"battle\",\n",
        "    \"bedCount\": \"beds\",\n",
        "    \"bird\": \"bird\",\n",
        "    \"birthName\": \"surname\",\n",
        "    \"birthPlace\": \"birthplace\",\n",
        "    \"broadcastedBy\": \"broadcast\",\n",
        "    \"buildingStartDate\": \"builds\",\n",
        "    \"buildingType\": \"Building\",\n",
        "    \"campus\": \"campus\",\n",
        "    \"capital\": \"capital\",\n",
        "    \"category\": \"category\",\n",
        "    \"chairman\": \"chairman\",\n",
        "    \"chairmanTitle\": \"Chairman\",\n",
        "    \"chairperson\": \"chair\",\n",
        "    \"champions\": \"champion\",\n",
        "    \"chancellor\": \"chancellor\",\n",
        "    \"chief\": \"chief\",\n",
        "    \"child\": \"child\",\n",
        "    \"city\": \"city\",\n",
        "    \"cityServed\": \"served\",\n",
        "    \"class\": \"class\",\n",
        "    \"club\": \"club\",\n",
        "    \"commander\": \"commander\",\n",
        "    \"compete in\": \"compete\",\n",
        "    \"completionDate\": \"completion\",\n",
        "    \"country\": \"country\",\n",
        "    # \"country\": \"nation\",\n",
        "    \"countySeat\": \"seat\",\n",
        "    \"course\": \"course\",\n",
        "    \"creator\": \"creator\",\n",
        "    \"creatorOfDish\": \"dishes\",\n",
        "    \"crewMembers\": \"crews\",\n",
        "    \"currency\": \"currency\",\n",
        "    \"currentTenants\": \"tenants\",\n",
        "    \"dean\": \"dean\",\n",
        "    \"deathPlace\": \"death\",\n",
        "    \"dedicatedTo\": \"dedicated\",\n",
        "    \"demonym\": \"evil\",\n",
        "    \"designer\": \"designer\",\n",
        "    \"developer\": \"developer\",\n",
        "    \"director\": \"director\",\n",
        "    \"dishVariation\": \"dish\",\n",
        "    \"distributor\": \"distributor\",\n",
        "    \"district\": \"district\",\n",
        "    \"division\": \"division\",\n",
        "    \"doctoralAdvisor\": \"advisor\",\n",
        "    \"editor\": \"editor\",\n",
        "    \"elevationAboveTheSeaLevel\": \"above\",\n",
        "    \"elevationAboveTheSeaLevel_(in_feet)\": \"feet\",\n",
        "    \"elevationAboveTheSeaLevel_(in_metres)\": \"metre\",\n",
        "    \"established\": \"establish\",\n",
        "    # \"ethnicGroup\": \"group\",\n",
        "    \"ethnicGroup\": \"ethnic\",\n",
        "    \"ethnicGroups\": \"Ethnic\",\n",
        "    \"family\": \"family\",\n",
        "    \"fat\": \"fat\",\n",
        "    \"firstAppearanceInFilm\": \"film\",\n",
        "    \"firstPublicationYear\": \"publication\",\n",
        "    \"floorArea\": \"floor\",\n",
        "    \"floorCount\": \"floors\",\n",
        "    \"followedBy\": \"follow\",\n",
        "    \"fossil\": \"fossil\",\n",
        "    \"foundationPlace\": \"founded\",\n",
        "    \"foundedBy\": \"Founder\",\n",
        "    \"founder\": \"founder\",\n",
        "    \"fullName\": \"name\",\n",
        "    \"fullname\": \"Name\",\n",
        "    \"gemstone\": \"cameo\",\n",
        "    \"genre\": \"Style\",\n",
        "    \"literaryGenre\":\"genre\",\n",
        "    \"genus\": \"genus\",\n",
        "    \"governingBody\": \"governing\",\n",
        "    \"governmentType\": \"government\",\n",
        "    \"ground\": \"ground\",\n",
        "    \"has to its north\": \"north\",\n",
        "    \"has to its northeast\": \"northeast\",\n",
        "    \"has to its northwest\": \"northwest\",\n",
        "    \"has to its southeast\": \"southeast\",\n",
        "    \"has to its southwest\": \"southwest\",\n",
        "    \"has to its west\": \"west\",\n",
        "    \"headquarter\": \"quarter\",\n",
        "    \"headquarters\": \"quarters\",\n",
        "    \"height\": \"height\",\n",
        "    \"higher\": \"higher\",\n",
        "    \"hometown\": \"hometown\",\n",
        "    \"hubAirport\": \"airport\",\n",
        "    \"inaugurationDate\": \"inauguration\",\n",
        "    \"influencedBy\": \"influenced\",\n",
        "    # \"ingredient\": \"component\",\n",
        "    \"ingredient\": \"ingredient\",\n",
        "    \"isPartOf\": \"part\",\n",
        "    \"jurisdiction\": \"jurisdiction\",\n",
        "    \"keyPerson\": \"key\",\n",
        "    \"language\": \"language\",\n",
        "    \"languages\": \"languages\",\n",
        "    \"largestCity\": \"largest\",\n",
        "    \"latinName\": \"Latin\",\n",
        "    \"leader\": \"lead\",\n",
        "    \"leaderName\": \"leader\",\n",
        "    \"leaderParty\": \"party\",\n",
        "    \"leaderTitle\": \"boss\",\n",
        "    \"league\": \"league\",\n",
        "    \"legislature\": \"legislature\",\n",
        "    \"location\": \"location\",\n",
        "    \"locationCity\": \"locate\",\n",
        "    \"mainIngredients\": \"ingredients\",\n",
        "    \"manager\": \"manager\",\n",
        "    \"material\": \"material\",\n",
        "    \"mayor\": \"mayor\",\n",
        "    \"mediaType\": \"media\",\n",
        "    \"motto\": \"motto\",\n",
        "    \"municipality\": \"municipality\",\n",
        "    \"nationality\": \"nationality\",\n",
        "    \"nativeName\": \"native\",\n",
        "    \"nearestCity\": \"nearest\",\n",
        "    \"neighboringMunicipality\": \"neighbor\",\n",
        "    \"nickname\": \"nickname\",\n",
        "    \"notableWork\": \"work\",\n",
        "    \"numberOfMembers\": \"members\",\n",
        "    \"numberOfPages\": \"pages\",\n",
        "    \"numberOfPostgraduateStudents\": \"postgraduate\",\n",
        "    \"numberOfRooms\": \"room\",\n",
        "    \"numberOfStudents\": \"students\",\n",
        "    \"numberOfUndergraduateStudents\": \"undergraduate\",\n",
        "    \"occupation\": \"occupation\",\n",
        "    \"officialLanguage\": \"Language\",\n",
        "    \"officialSchoolColour\": \"colour\",\n",
        "    \"operatingOrganisation\": \"operating\",\n",
        "    \"operator\": \"operator\",\n",
        "    \"order\": \"order\",\n",
        "    \"outlookRanking\": \"ranking\",\n",
        "    \"owner\": \"owner\",\n",
        "    \"owningOrganisation\": \"owning\",\n",
        "    \"parentCompany\": \"company\",\n",
        "    \"part\": \"Part\",\n",
        "    \"partsType\":\"parts\",\n",
        "    \"patronSaint\": \"patron\",\n",
        "    \"placeOfBirth\": 15,\n",
        "    \"placeOfDeath\": 16,\n",
        "    \"populationDensity\": \"density\",\n",
        "    \"populationTotal\": \"population\",\n",
        "    \"postalCode\": \"postal\",\n",
        "    \"precededBy\": \"preceded\",\n",
        "    \"president\": \"president\",\n",
        "    \"product\": \"product\",\n",
        "    \"protein\": \"protein\",\n",
        "    \"publisher\": \"publisher\",\n",
        "    \"rector\": \"rector\",\n",
        "    \"region\": \"region\",\n",
        "    \"regionServed\": \"Region\",\n",
        "    \"religion\": \"religion\",\n",
        "    \"representative\": \"representative\",\n",
        "    \"residence\": \"residence\",\n",
        "    \"river\": \"river\",\n",
        "    \"runwayLength\": \"length\",\n",
        "    \"runwayName\": \"runway\",\n",
        "    \"season\": \"season\",\n",
        "    \"senators\": \"senator\",\n",
        "    \"series\": \"series\",\n",
        "    \"served as Chief of the Astronaut Office in\": \"astronaut\",\n",
        "    \"servingTemperature\": \"temperature\",\n",
        "    \"significantBuilding\": \"building\",\n",
        "    \"significantProject\": \"project\",\n",
        "    \"spokenIn\": \"spoken\",\n",
        "    \"sportsGoverningBody\": \"sport\",\n",
        "    \"sportsOffered\": \"sports\",\n",
        "    \"starring\": \"starring\",\n",
        "    \"state\": \"state\",\n",
        "    \"tenant\": \"tenant\",\n",
        "    \"title\": \"title\",\n",
        "    \"transportAircraft\": \"transport\",\n",
        "    \"voice\": \"voice\",\n",
        "    \"was a crew member of\": \"crew\",\n",
        "    \"was given the 'Technical Campus' status by\": \"technical\",\n",
        "    \"was selected by NASA\": \"NASA\",\n",
        "    \"year\": \"year\",\n",
        "    \"yearOfConstruction\": \"construction\",\n",
        "    \"youthclub\": \"youth\"\n",
        "}\n",
        "\n",
        "webnlg_rel2clearrep = {\n",
        "    \"1st_runway_LengthFeet\": \"first feet\",\n",
        "    \"1st_runway_LengthMetre\": \"first metre\",\n",
        "    \"1st_runway_Number\": \"first number\",\n",
        "    \"1st_runway_SurfaceType\": \"firset surface\",\n",
        "    \"2nd_runway_SurfaceType\": \"second surface\",\n",
        "    \"3rd_runway_LengthFeet\": \"third feet\",\n",
        "    \"3rd_runway_SurfaceType\": \"third surface\",\n",
        "    \"4th_runway_LengthFeet\": \"forth feet\",\n",
        "    \"5th_runway_Number\": \"fivth number\",\n",
        "    \"CODEN_code\": \"CODEN\",\n",
        "    \"EISSN_number\": \"EISSN\",\n",
        "    \"IATA_Location_Identifier\": \"IATA\",\n",
        "    \"ICAO_Location_Identifier\": \"ICAO\",\n",
        "    \"ISBN_number\": \"ISBN\",\n",
        "    \"ISSN_number\": \"ISSN\",\n",
        "    \"LCCN_number\": \"LCCN\",\n",
        "    \"LibraryofCongressClassification\": \"LCC\",\n",
        "    \"OCLC_number\": \"OCLC\",\n",
        "    \"ReferenceNumber in the National Register of Historic Places\": \"reference NRHP\",\n",
        "    \"abbreviation\": \"abbreviation\",\n",
        "    \"academicDiscipline\": \"academicDiscipline\",\n",
        "    \"academicStaffSize\": \"academicStaffSize\",\n",
        "    \"added to the National Register of Historic Places\": \"add NRHP\",\n",
        "    \"address\": \"address\",\n",
        "    \"administrativeArrondissement\": \"administrativeArrondissement\",\n",
        "    \"administrativeCounty\": \"administrativeCounty\",\n",
        "    \"affiliation\": \"affiliation\",\n",
        "    \"affiliations\": \"affiliate\",\n",
        "    \"aircraftFighter\": \"aircraftFighter\",\n",
        "    \"aircraftHelicopter\": \"aircraftHelicopter\",\n",
        "    \"almaMater\": \"almaMater\",\n",
        "    \"alternativeName\": \"alternativeName\",\n",
        "    \"anthem\": \"anthem\",\n",
        "    \"architect\": \"architect\",\n",
        "    \"architecturalStyle\": \"architecturalStyle\",\n",
        "    \"architecture\": \"architecture\",\n",
        "    \"areaCode\": \"area\",\n",
        "    \"areaOfLand\": \"areaOfLand\",\n",
        "    \"areaOfWater\": \"areaOfWater\",\n",
        "    \"areaTotal\": \"areaTotal\",\n",
        "    \"attackAircraft\": \"attackAircraft\",\n",
        "    \"author\": \"author\",\n",
        "    \"award\": \"award\",\n",
        "    \"awards\": \"awards\",\n",
        "    \"backup pilot\": \"backup pilot\",\n",
        "    \"battles\": \"battle\",\n",
        "    \"bedCount\": \"bedCount\",\n",
        "    \"bird\": \"bird\",\n",
        "    \"birthName\": \"surname\",\n",
        "    \"birthPlace\": \"birthplace\",\n",
        "    \"broadcastedBy\": \"broadcast\",\n",
        "    \"buildingStartDate\": \"builds\",\n",
        "    \"buildingType\": \"Building\",\n",
        "    \"campus\": \"campus\",\n",
        "    \"capital\": \"capital\",\n",
        "    \"category\": \"category\",\n",
        "    \"chairman\": \"chairman\",\n",
        "    \"chairmanTitle\": \"chairmanTitle\",\n",
        "    \"chairperson\": \"chairperson\",\n",
        "    \"champions\": \"champion\",\n",
        "    \"chancellor\": \"chancellor\",\n",
        "    \"chief\": \"chief\",\n",
        "    \"child\": \"child\",\n",
        "    \"city\": \"city\",\n",
        "    \"cityServed\": \"cityServed\",\n",
        "    \"class\": \"class\",\n",
        "    \"club\": \"club\",\n",
        "    \"commander\": \"commander\",\n",
        "    \"compete in\": \"compete\",\n",
        "    \"completionDate\": \"completionDate\",\n",
        "    \"country\": \"country\",\n",
        "    # \"country\": \"nation\",\n",
        "    \"countySeat\": \"countySeat\",\n",
        "    \"course\": \"course\",\n",
        "    \"creator\": \"creator\",\n",
        "    \"creatorOfDish\": \"creatorOfDish\",\n",
        "    \"crewMembers\": \"crewMembers\",\n",
        "    \"currency\": \"currency\",\n",
        "    \"currentTenants\": \"currentTenants\",\n",
        "    \"dean\": \"dean\",\n",
        "    \"deathPlace\": \"deathPlace\",\n",
        "    \"dedicatedTo\": \"dedicated\",\n",
        "    \"demonym\": \"evil\",\n",
        "    \"designer\": \"designer\",\n",
        "    \"developer\": \"developer\",\n",
        "    \"director\": \"director\",\n",
        "    \"dishVariation\": \"dish\",\n",
        "    \"distributor\": \"distributor\",\n",
        "    \"district\": \"district\",\n",
        "    \"division\": \"division\",\n",
        "    \"doctoralAdvisor\": \"doctoralAdvisor\",\n",
        "    \"editor\": \"editor\",\n",
        "    \"elevationAboveTheSeaLevel\": \"above sea level\",\n",
        "    \"elevationAboveTheSeaLevel_(in_feet)\": \"feet above sea level\",\n",
        "    \"elevationAboveTheSeaLevel_(in_metres)\": \"metre above sea levle\",\n",
        "    \"established\": \"establish\",\n",
        "    # \"ethnicGroup\": \"group\",\n",
        "    \"ethnicGroup\": \"ethnic group\",\n",
        "    \"ethnicGroups\": \"ethnic groups\",\n",
        "    \"family\": \"family\",\n",
        "    \"fat\": \"fat\",\n",
        "    \"firstAppearanceInFilm\": \"appearance film\",\n",
        "    \"firstPublicationYear\": \"publication year\",\n",
        "    \"floorArea\": \"floor\",\n",
        "    \"floorCount\": \"floors\",\n",
        "    \"followedBy\": \"follow\",\n",
        "    \"fossil\": \"fossil\",\n",
        "    \"foundationPlace\": \"foundation\",\n",
        "    \"foundedBy\": \"founded\",\n",
        "    \"founder\": \"founder\",\n",
        "    \"fullName\": \"Name\",\n",
        "    \"fullname\": \"name\",\n",
        "    \"gemstone\": \"cameo\",\n",
        "    \"genre\": \"genre\",\n",
        "    \"literaryGenre\":\"literaryGenre\",\n",
        "    \"genus\": \"genus\",\n",
        "    \"governingBody\": \"governing\",\n",
        "    \"governmentType\": \"government\",\n",
        "    \"ground\": \"ground\",\n",
        "    \"has to its north\": \"north\",\n",
        "    \"has to its northeast\": \"northeast\",\n",
        "    \"has to its northwest\": \"northwest\",\n",
        "    \"has to its southeast\": \"southeast\",\n",
        "    \"has to its southwest\": \"southwest\",\n",
        "    \"has to its west\": \"west\",\n",
        "    \"headquarter\": \"quarter\",\n",
        "    \"headquarters\": \"quarters\",\n",
        "    \"height\": \"height\",\n",
        "    \"higher\": \"higher\",\n",
        "    \"hometown\": \"hometown\",\n",
        "    \"hubAirport\": \"airport\",\n",
        "    \"inaugurationDate\": \"inauguration\",\n",
        "    \"influencedBy\": \"influenced\",\n",
        "    # \"ingredient\": \"component\",\n",
        "    \"ingredient\": \"ingredient\",\n",
        "    \"isPartOf\": \"part of\",\n",
        "    \"jurisdiction\": \"jurisdiction\",\n",
        "    \"keyPerson\": \"key person\",\n",
        "    \"language\": \"language\",\n",
        "    \"languages\": \"languages\",\n",
        "    \"largestCity\": \"largest city\",\n",
        "    \"latinName\": \"latin name\",\n",
        "    \"leader\": \"lead\",\n",
        "    \"leaderName\": \"leader\",\n",
        "    \"leaderParty\": \"leader party\",\n",
        "    \"leaderTitle\": \"leader title\",\n",
        "    \"league\": \"league\",\n",
        "    \"legislature\": \"legislature\",\n",
        "    \"location\": \"location\",\n",
        "    \"locationCity\": \"locate\",\n",
        "    \"mainIngredients\": \"main ingredient\",\n",
        "    \"manager\": \"manager\",\n",
        "    \"material\": \"material\",\n",
        "    \"mayor\": \"mayor\",\n",
        "    \"mediaType\": \"media type\",\n",
        "    \"motto\": \"motto\",\n",
        "    \"municipality\": \"municipality\",\n",
        "    \"nationality\": \"nationality\",\n",
        "    \"nativeName\": \"native\",\n",
        "    \"nearestCity\": \"nearest\",\n",
        "    \"neighboringMunicipality\": \"neighbor\",\n",
        "    \"nickname\": \"nickname\",\n",
        "    \"notableWork\": \"work\",\n",
        "    \"numberOfMembers\": \"members\",\n",
        "    \"numberOfPages\": \"pages number\",\n",
        "    \"numberOfPostgraduateStudents\": \"postgraduate number\",\n",
        "    \"numberOfRooms\": \"room number\",\n",
        "    \"numberOfStudents\": \"students number\",\n",
        "    \"numberOfUndergraduateStudents\": \"undergraduate number\",\n",
        "    \"occupation\": \"occupation\",\n",
        "    \"officialLanguage\": \"offical language\",\n",
        "    \"officialSchoolColour\": \"offical colour\",\n",
        "    \"operatingOrganisation\": \"operatingOrganisation\",\n",
        "    \"operator\": \"operator\",\n",
        "    \"order\": \"order\",\n",
        "    \"outlookRanking\": \"ranking\",\n",
        "    \"owner\": \"owner\",\n",
        "    \"owningOrganisation\": \"owning\",\n",
        "    \"parentCompany\": \"company\",\n",
        "    \"part\": \"part\",\n",
        "    \"partsType\":\"part type\",\n",
        "    \"patronSaint\": \"patron\",\n",
        "    \"placeOfBirth\": \"placeOfBirth\",\n",
        "    \"placeOfDeath\": \"placeOfDeath\",\n",
        "    \"populationDensity\": \"density\",\n",
        "    \"populationTotal\": \"population number\",\n",
        "    \"postalCode\": \"postal\",\n",
        "    \"precededBy\": \"preceded\",\n",
        "    \"president\": \"president\",\n",
        "    \"product\": \"product\",\n",
        "    \"protein\": \"protein\",\n",
        "    \"publisher\": \"publisher\",\n",
        "    \"rector\": \"rector\",\n",
        "    \"region\": \"region\",\n",
        "    \"regionServed\": \"Region\",\n",
        "    \"religion\": \"religion\",\n",
        "    \"representative\": \"representative\",\n",
        "    \"residence\": \"residence\",\n",
        "    \"river\": \"river\",\n",
        "    \"runwayLength\": \"length\",\n",
        "    \"runwayName\": \"runway\",\n",
        "    \"season\": \"season\",\n",
        "    \"senators\": \"senator\",\n",
        "    \"series\": \"series\",\n",
        "    \"served as Chief of the Astronaut Office in\": \"chief astronaut\",\n",
        "    \"servingTemperature\": \"temperature\",\n",
        "    \"significantBuilding\": \"significant building\",\n",
        "    \"significantProject\": \"significant project\",\n",
        "    \"spokenIn\": \"spoken\",\n",
        "    \"sportsGoverningBody\": \"sport govern\",\n",
        "    \"sportsOffered\": \"sports offer\",\n",
        "    \"starring\": \"starring\",\n",
        "    \"state\": \"state\",\n",
        "    \"tenant\": \"tenant\",\n",
        "    \"title\": \"title\",\n",
        "    \"transportAircraft\": \"transport aircraft\",\n",
        "    \"voice\": \"voice\",\n",
        "    \"was a crew member of\": \"crew member\",\n",
        "    \"was given the 'Technical Campus' status by\": \"technical campus\",\n",
        "    \"was selected by NASA\": \"NASA\",\n",
        "    \"year\": \"year\",\n",
        "    \"yearOfConstruction\": \"construction year\",\n",
        "    \"youthclub\": \"youth club\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import warnings\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from packaging import version\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "\n",
        "from transformers.activations import ACT2FN\n",
        "from transformers.file_utils import (\n",
        "    ModelOutput,\n",
        "    add_code_sample_docstrings,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    replace_return_docstrings,\n",
        ")\n",
        "from transformers.modeling_outputs import (\n",
        "    CausalLMOutputWithCrossAttentions,\n",
        "    MaskedLMOutput,\n",
        "    MultipleChoiceModelOutput,\n",
        "    NextSentencePredictorOutput,\n",
        "    QuestionAnsweringModelOutput,\n",
        "    SequenceClassifierOutput,\n",
        "    TokenClassifierOutput,\n",
        ")\n",
        "from transformers.modeling_utils import (\n",
        "    PreTrainedModel,\n",
        "    apply_chunking_to_forward,\n",
        "    find_pruneable_heads_and_indices,\n",
        "    prune_linear_layer,\n",
        ")\n",
        "from transformers.utils import logging\n",
        "from transformers import BertConfig\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "_CHECKPOINT_FOR_DOC = \"bert-base-uncased\"\n",
        "_CONFIG_FOR_DOC = \"BertConfig\"\n",
        "_TOKENIZER_FOR_DOC = \"BertTokenizer\"\n",
        "\n",
        "BERT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"bert-base-uncased\",\n",
        "    \"bert-large-uncased\",\n",
        "    \"bert-base-cased\",\n",
        "    \"bert-large-cased\",\n",
        "    \"bert-base-multilingual-uncased\",\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    \"bert-base-chinese\",\n",
        "    \"bert-base-german-cased\",\n",
        "    \"bert-large-uncased-whole-word-masking\",\n",
        "    \"bert-large-cased-whole-word-masking\",\n",
        "    \"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
        "    \"bert-large-cased-whole-word-masking-finetuned-squad\",\n",
        "    \"bert-base-cased-finetuned-mrpc\",\n",
        "    \"bert-base-german-dbmdz-cased\",\n",
        "    \"bert-base-german-dbmdz-uncased\",\n",
        "    \"cl-tohoku/bert-base-japanese\",\n",
        "    \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
        "    \"cl-tohoku/bert-base-japanese-char\",\n",
        "    \"cl-tohoku/bert-base-japanese-char-whole-word-masking\",\n",
        "    \"TurkuNLP/bert-base-finnish-cased-v1\",\n",
        "    \"TurkuNLP/bert-base-finnish-uncased-v1\",\n",
        "    \"wietsedv/bert-base-dutch-cased\",\n",
        "    # See all BERT models at https://huggingface.co/models?filter=bert\n",
        "]\n",
        "\n",
        "@dataclass\n",
        "class BaseModelOutputWithPastAndCrossAttentions(ModelOutput):\n",
        "    \"\"\"\n",
        "    Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).\n",
        "\n",
        "    Args:\n",
        "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
        "            Sequence of hidden-states at the output of the last layer of the model.\n",
        "\n",
        "            If :obj:`past_key_values` is used only the last hidden-state of the sequences of shape :obj:`(batch_size,\n",
        "            1, hidden_size)` is output.\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
        "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
        "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
        "            ``config.is_encoder_decoder=True`` 2 additional tensors of shape :obj:`(batch_size, num_heads,\n",
        "            encoder_sequence_length, embed_size_per_head)`.\n",
        "\n",
        "            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n",
        "            ``config.is_encoder_decoder=True`` in the cross-attention blocks) that can be used (see\n",
        "            :obj:`past_key_values` input) to speed up sequential decoding.\n",
        "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
        "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
        "\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
        "            sequence_length, sequence_length)`.\n",
        "\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
        "            heads.\n",
        "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` and ``config.add_cross_attention=True`` is passed or when ``config.output_attentions=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
        "            sequence_length, sequence_length)`.\n",
        "\n",
        "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
        "            weighted average in the cross-attention heads.\n",
        "    \"\"\"\n",
        "\n",
        "    last_hidden_state: torch.FloatTensor = None\n",
        "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions_scores: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "\n",
        "@dataclass\n",
        "class BaseModelOutputWithPoolingAndCrossAttentions(ModelOutput):\n",
        "    \"\"\"\n",
        "    Base class for model's outputs that also contains a pooling of the last hidden states.\n",
        "\n",
        "    Args:\n",
        "        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):\n",
        "            Sequence of hidden-states at the output of the last layer of the model.\n",
        "        pooler_output (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, hidden_size)`):\n",
        "            Last layer hidden-state of the first token of the sequence (classification token) after further processing\n",
        "            through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns\n",
        "            the classification token after processing through a linear layer and a tanh activation function. The linear\n",
        "            layer weights are trained from the next sentence prediction (classification) objective during pretraining.\n",
        "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
        "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
        "\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
        "            sequence_length, sequence_length)`.\n",
        "\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
        "            heads.\n",
        "        cross_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` and ``config.add_cross_attention=True`` is passed or when ``config.output_attentions=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
        "            sequence_length, sequence_length)`.\n",
        "\n",
        "            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n",
        "            weighted average in the cross-attention heads.\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n",
        "            Tuple of :obj:`tuple(torch.FloatTensor)` of length :obj:`config.n_layers`, with each tuple having 2 tensors\n",
        "            of shape :obj:`(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
        "            ``config.is_encoder_decoder=True`` 2 additional tensors of shape :obj:`(batch_size, num_heads,\n",
        "            encoder_sequence_length, embed_size_per_head)`.\n",
        "\n",
        "            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n",
        "            ``config.is_encoder_decoder=True`` in the cross-attention blocks) that can be used (see\n",
        "            :obj:`past_key_values` input) to speed up sequential decoding.\n",
        "    \"\"\"\n",
        "\n",
        "    last_hidden_state: torch.FloatTensor = None\n",
        "    pooler_output: torch.FloatTensor = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions_scores: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "\n",
        "def load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n",
        "    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n",
        "    try:\n",
        "        import re\n",
        "\n",
        "        import numpy as np\n",
        "        import tensorflow as tf\n",
        "    except ImportError:\n",
        "        logger.error(\n",
        "            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n",
        "            \"https://www.tensorflow.org/install/ for installation instructions.\"\n",
        "        )\n",
        "        raise\n",
        "    tf_path = os.path.abspath(tf_checkpoint_path)\n",
        "    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n",
        "    # Load weights from TF model\n",
        "    init_vars = tf.train.list_variables(tf_path)\n",
        "    names = []\n",
        "    arrays = []\n",
        "    for name, shape in init_vars:\n",
        "        logger.info(f\"Loading TF weight {name} with shape {shape}\")\n",
        "        array = tf.train.load_variable(tf_path, name)\n",
        "        names.append(name)\n",
        "        arrays.append(array)\n",
        "\n",
        "    for name, array in zip(names, arrays):\n",
        "        name = name.split(\"/\")\n",
        "        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n",
        "        # which are not required for using pretrained model\n",
        "        if any(\n",
        "            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n",
        "            for n in name\n",
        "        ):\n",
        "            logger.info(f\"Skipping {'/'.join(name)}\")\n",
        "            continue\n",
        "        pointer = model\n",
        "        for m_name in name:\n",
        "            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n",
        "                scope_names = re.split(r\"_(\\d+)\", m_name)\n",
        "            else:\n",
        "                scope_names = [m_name]\n",
        "            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n",
        "                pointer = getattr(pointer, \"weight\")\n",
        "            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n",
        "                pointer = getattr(pointer, \"bias\")\n",
        "            elif scope_names[0] == \"output_weights\":\n",
        "                pointer = getattr(pointer, \"weight\")\n",
        "            elif scope_names[0] == \"squad\":\n",
        "                pointer = getattr(pointer, \"classifier\")\n",
        "            else:\n",
        "                try:\n",
        "                    pointer = getattr(pointer, scope_names[0])\n",
        "                except AttributeError:\n",
        "                    logger.info(f\"Skipping {'/'.join(name)}\")\n",
        "                    continue\n",
        "            if len(scope_names) >= 2:\n",
        "                num = int(scope_names[1])\n",
        "                pointer = pointer[num]\n",
        "        if m_name[-11:] == \"_embeddings\":\n",
        "            pointer = getattr(pointer, \"weight\")\n",
        "        elif m_name == \"kernel\":\n",
        "            array = np.transpose(array)\n",
        "        try:\n",
        "            if pointer.shape != array.shape:\n",
        "                raise ValueError(f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\")\n",
        "        except AssertionError as e:\n",
        "            e.args += (pointer.shape, array.shape)\n",
        "            raise\n",
        "        logger.info(f\"Initialize PyTorch weight {name}\")\n",
        "        pointer.data = torch.from_numpy(array)\n",
        "    return model\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
        "        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):\n",
        "            self.register_buffer(\n",
        "                \"token_type_ids\",\n",
        "                torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n",
        "                persistent=False,\n",
        "            )\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
        "    ):\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
        "\n",
        "        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n",
        "        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n",
        "        # issue #5664\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "        if self.position_embedding_type == \"absolute\":\n",
        "            position_embeddings = self.position_embeddings(position_ids)\n",
        "            embeddings += position_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({config.num_attention_heads})\"\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "        self.is_decoder = config.is_decoder\n",
        "        self.is_prompt_rel = False\n",
        "        self.num_rels = config.num_rels\n",
        "        if self.is_prompt_rel:\n",
        "            self.rel_embedding = nn.Embedding(config.num_rels, config.hidden_size)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "        output_attentions_scores=False,\n",
        "    ):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        batch_size, token_len, emb_size = hidden_states.size()\n",
        "        if self.is_prompt_rel:\n",
        "            re_embedding = self.rel_embedding(torch.tensor(range(0, self.num_rels)).to(hidden_states.device))\n",
        "            ex_re_embedding = re_embedding.unsqueeze(0).repeat(batch_size,1,1)\n",
        "            key_layer = self.transpose_for_scores(self.key(torch.cat((hidden_states, ex_re_embedding), -2)))\n",
        "            value_layer = self.transpose_for_scores(self.value(torch.cat((hidden_states, ex_re_embedding),-2)))\n",
        "            # query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        else:\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "            is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "            if is_cross_attention and past_key_value is not None:\n",
        "                # reuse k,v, cross_attentions\n",
        "                key_layer = past_key_value[0]\n",
        "                value_layer = past_key_value[1]\n",
        "                attention_mask = encoder_attention_mask\n",
        "            elif is_cross_attention:\n",
        "                key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "                value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "                attention_mask = encoder_attention_mask\n",
        "            elif past_key_value is not None:\n",
        "                key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "                value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "                key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "                value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "            else:\n",
        "                key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "                value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "            query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "            if self.is_decoder:\n",
        "                # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "                # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "                # key/value_states (first \"if\" case)\n",
        "                # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "                # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "                # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "                # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "                past_key_value = (key_layer, value_layer)\n",
        "\n",
        "            # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            seq_length = hidden_states.size()[1]\n",
        "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "            if self.is_prompt_rel:\n",
        "                rel_attention_mask = torch.zeros(self.num_rels).to(hidden_states.device).unsqueeze(0).unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1, 1)\n",
        "                attention_scores = attention_scores + torch.cat((attention_mask, rel_attention_mask), -1)\n",
        "            else:\n",
        "                attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        if output_attentions and output_attentions_scores:\n",
        "            outputs = (context_layer, attention_probs, attention_scores)\n",
        "        elif output_attentions_scores and not output_attentions:\n",
        "            outputs = (context_layer, attention_scores)\n",
        "        elif output_attentions and not output_attentions_scores:\n",
        "            outputs = (context_layer, attention_scores)\n",
        "        else:\n",
        "            outputs = (context_layer,)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        heads, index = find_pruneable_heads_and_indices(\n",
        "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
        "        )\n",
        "\n",
        "        # Prune linear layers\n",
        "        self.self.query = prune_linear_layer(self.self.query, index)\n",
        "        self.self.key = prune_linear_layer(self.self.key, index)\n",
        "        self.self.value = prune_linear_layer(self.self.value, index)\n",
        "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "        # Update hyper params and store pruned heads\n",
        "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
        "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "        output_attentions_scores=False\n",
        "    ):\n",
        "        self_outputs = self.self(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions,\n",
        "            output_attentions_scores\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "        self.seq_len_dim = 1\n",
        "        self.attention = BertAttention(config)\n",
        "        self.is_decoder = config.is_decoder\n",
        "        self.add_cross_attention = config.add_cross_attention\n",
        "        if self.add_cross_attention:\n",
        "            if not self.is_decoder:\n",
        "                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n",
        "            self.crossattention = BertAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "        output_attentions_scores=False\n",
        "    ):\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        self_attention_outputs = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_attentions_scores=output_attentions_scores,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "\n",
        "        # if decoder, the last output is tuple of self-attn cache\n",
        "        if self.is_decoder:\n",
        "            outputs = self_attention_outputs[1:-1]\n",
        "            present_key_value = self_attention_outputs[-1]\n",
        "        else:\n",
        "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "\n",
        "        cross_attn_present_key_value = None\n",
        "        if self.is_decoder and encoder_hidden_states is not None:\n",
        "            if not hasattr(self, \"crossattention\"):\n",
        "                raise ValueError(\n",
        "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
        "                )\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            cross_attention_outputs = self.crossattention(\n",
        "                attention_output,\n",
        "                attention_mask,\n",
        "                head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                cross_attn_past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "            attention_output = cross_attention_outputs[0]\n",
        "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
        "\n",
        "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
        "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        layer_output = apply_chunking_to_forward(\n",
        "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
        "        )\n",
        "        outputs = (layer_output,) + outputs\n",
        "\n",
        "        # if decoder, return the attn key/values as the last output\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (present_key_value,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def feed_forward_chunk(self, attention_output):\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.gradient_checkpointing = False\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=False,\n",
        "        output_attentions_scores=False,\n",
        "        output_hidden_states=False,\n",
        "        return_dict=True,\n",
        "    ):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_self_attentions_scores = () if output_attentions_scores else None\n",
        "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "\n",
        "                if use_cache:\n",
        "                    logger.warning(\n",
        "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
        "                    )\n",
        "                    use_cache = False\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, past_key_value, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                    output_attentions_scores,\n",
        "                )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[-1],)\n",
        "            if output_attentions and output_attentions_scores:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "                all_self_attentions_scores = all_self_attentions_scores + (layer_outputs[2],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "            elif output_attentions_scores and not output_attentions:\n",
        "                all_self_attentions_scores = all_self_attentions_scores + (layer_outputs[1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "            elif output_attentions and not output_attentions_scores:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [\n",
        "                    hidden_states,\n",
        "                    next_decoder_cache,\n",
        "                    all_hidden_states,\n",
        "                    all_self_attentions,\n",
        "                    all_self_attentions_scores,\n",
        "                    all_cross_attentions,\n",
        "                ]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_decoder_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "            attentions_scores=all_self_attentions_scores,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class BertPredictionHeadTransform(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.transform_act_fn = config.hidden_act\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLMPredictionHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.transform = BertPredictionHeadTransform(config)\n",
        "\n",
        "        # The output weights are the same as the input embeddings, but there is\n",
        "        # an output-only bias for each token.\n",
        "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "\n",
        "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
        "        self.decoder.bias = self.bias\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.transform(hidden_states)\n",
        "        hidden_states = self.decoder(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOnlyMLMHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.predictions = BertLMPredictionHead(config)\n",
        "\n",
        "    def forward(self, sequence_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        return prediction_scores\n",
        "\n",
        "\n",
        "class BertOnlyNSPHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, pooled_output):\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return seq_relationship_score\n",
        "\n",
        "\n",
        "class BertPreTrainingHeads(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.predictions = BertLMPredictionHead(config)\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, sequence_output, pooled_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return prediction_scores, seq_relationship_score\n",
        "\n",
        "\n",
        "class BertPreTrainedModel(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
        "    models.\n",
        "    \"\"\"\n",
        "\n",
        "    config_class = BertConfig\n",
        "    load_tf_weights = load_tf_weights_in_bert\n",
        "    base_model_prefix = \"bert\"\n",
        "    supports_gradient_checkpointing = True\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize the weights\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def _set_gradient_checkpointing(self, module, value=False):\n",
        "        if isinstance(module, BertEncoder):\n",
        "            module.gradient_checkpointing = value\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class BertForPreTrainingOutput(ModelOutput):\n",
        "    \"\"\"\n",
        "    Output type of :class:`~transformers.BertForPreTraining`.\n",
        "\n",
        "    Args:\n",
        "        loss (`optional`, returned when ``labels`` is provided, ``torch.FloatTensor`` of shape :obj:`(1,)`):\n",
        "            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n",
        "            (classification) loss.\n",
        "        prediction_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n",
        "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
        "        seq_relationship_logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, 2)`):\n",
        "            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n",
        "            before SoftMax).\n",
        "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
        "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
        "\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape :obj:`(batch_size, num_heads,\n",
        "            sequence_length, sequence_length)`.\n",
        "\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
        "            heads.\n",
        "    \"\"\"\n",
        "\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    prediction_logits: torch.FloatTensor = None\n",
        "    seq_relationship_logits: torch.FloatTensor = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "\n",
        "\n",
        "\n",
        "BERT_START_DOCSTRING = r\"\"\"\n",
        "\n",
        "    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n",
        "    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n",
        "    pruning heads etc.)\n",
        "\n",
        "    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__\n",
        "    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to\n",
        "    general usage and behavior.\n",
        "\n",
        "    Parameters:\n",
        "        config (:class:`~transformers.BertConfig`): Model configuration class with all the parameters of the model.\n",
        "            Initializing with a config file does not load the weights associated with the model, only the\n",
        "            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n",
        "            weights.\n",
        "\"\"\"\n",
        "\n",
        "BERT_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`):\n",
        "            Indices of input sequence tokens in the vocabulary.\n",
        "\n",
        "            Indices can be obtained using :class:`~transformers.BertTokenizer`. See\n",
        "            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
        "            details.\n",
        "\n",
        "            `What are input IDs? <../glossary.html#input-ids>`__\n",
        "        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "\n",
        "            `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "        token_type_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in ``[0,\n",
        "            1]``:\n",
        "\n",
        "            - 0 corresponds to a `sentence A` token,\n",
        "            - 1 corresponds to a `sentence B` token.\n",
        "\n",
        "            `What are token type IDs? <../glossary.html#token-type-ids>`_\n",
        "        position_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,\n",
        "            config.max_position_embeddings - 1]``.\n",
        "\n",
        "            `What are position IDs? <../glossary.html#position-ids>`_\n",
        "        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n",
        "            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "\n",
        "        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`({0}, hidden_size)`, `optional`):\n",
        "            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n",
        "            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n",
        "            vectors than the model's internal embedding lookup matrix.\n",
        "        output_attentions (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (:obj:`bool`, `optional`):\n",
        "            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare Bert Model transformer outputting raw hidden-states without any specific head on top.\",\n",
        "    BERT_START_DOCSTRING,\n",
        ")\n",
        "class BertModel(BertPreTrainedModel):\n",
        "    \"\"\"\n",
        "\n",
        "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
        "    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
        "    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
        "    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
        "\n",
        "    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n",
        "    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n",
        "    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
        "    input to the forward pass.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, add_pooling_layer=True):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "\n",
        "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.word_embeddings\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embeddings.word_embeddings = value\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\"\n",
        "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
        "        class PreTrainedModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        processor_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_attentions_scores=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "\n",
        "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
        "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
        "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
        "        use_cache (:obj:`bool`, `optional`):\n",
        "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
        "            decoding (see :obj:`past_key_values`).\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_attentions_scores = output_attentions_scores if output_attentions_scores is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if self.config.is_decoder:\n",
        "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        else:\n",
        "            use_cache = False\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        batch_size, seq_length = input_shape\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
        "\n",
        "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
        "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
        "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "            if encoder_attention_mask is None:\n",
        "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "        else:\n",
        "            encoder_extended_attention_mask = None\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            past_key_values_length=past_key_values_length,\n",
        "        )\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_extended_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_attentions_scores=output_attentions_scores,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
        "            last_hidden_state=sequence_output,\n",
        "            pooler_output=pooled_output,\n",
        "            past_key_values=encoder_outputs.past_key_values,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "            attentions_scores=encoder_outputs.attentions_scores,\n",
        "            cross_attentions=encoder_outputs.cross_attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "yB7IbJtDGwK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import dataclasses\n",
        "from typing import Optional\n",
        "from torch import nn\n",
        "\n",
        "from transformers import (PreTrainedModel, BertPreTrainedModel, BertConfig,\n",
        "                          BertTokenizerFast)\n",
        "from transformers.file_utils import ModelOutput\n",
        "from transformers.models.bert.modeling_bert import BertOnlyMLMHead, BertOnlyNSPHead, BertForMaskedLM, BertLMHeadModel\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class UniRelOutput(ModelOutput):\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    head_preds: Optional[torch.FloatTensor] = None\n",
        "    tail_preds: Optional[torch.FloatTensor] = None\n",
        "    span_preds: Optional[torch.FloatTensor] = None\n",
        "\n",
        "class UniRelModel(BertPreTrainedModel):\n",
        "    \"\"\"\n",
        "    Model for learning Interaction Map\n",
        "    \"\"\"\n",
        "    def __init__(self, config, model_dir=None):\n",
        "        super(UniRelModel, self).__init__(config=config)\n",
        "        self.config = config\n",
        "        if model_dir is not None:\n",
        "            self.bert = BertModel.from_pretrained(model_dir, config=config)\n",
        "        else:\n",
        "            self.bert = BertModel(config)\n",
        "\n",
        "        # Easy debug\n",
        "        self.tokenizer = BertTokenizerFast.from_pretrained(\n",
        "            \"bert-base-cased\", do_basic_tokenize=False)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        # Abaltion experiment\n",
        "        if config.is_additional_att or config.is_separate_ablation:\n",
        "            self.key_linear = nn.Linear(768, 64)\n",
        "            self.value_linear = nn.Linear(768, 64)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        token_len_batch=None,\n",
        "        labels=None,\n",
        "        head_label=None,\n",
        "        tail_label=None,\n",
        "        span_label=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        tail_logits = None\n",
        "        # For span extraction\n",
        "        head_logits= None\n",
        "        span_logits = None\n",
        "        #\n",
        "        if not self.config.is_separate_ablation:\n",
        "            # Encoding the sentence and relations simultaneously, and using the inside Attention score\n",
        "            outputs = self.bert(\n",
        "                            input_ids=input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids,\n",
        "                            position_ids=position_ids,\n",
        "                            head_mask=head_mask,\n",
        "                            inputs_embeds=inputs_embeds,\n",
        "                            output_attentions=False,\n",
        "                            output_attentions_scores=True,\n",
        "                            output_hidden_states=output_hidden_states,\n",
        "                            return_dict=return_dict)\n",
        "            attentions_scores = outputs.attentions_scores[-1]\n",
        "            BATCH_SIZE, ATT_HEADS, ATT_LEN, _ = attentions_scores.size()\n",
        "            ATT_LAYERS = len(attentions_scores)\n",
        "            if self.config.test_data_type == \"unirel_span\":\n",
        "                head_logits = nn.Sigmoid()(\n",
        "                        attentions_scores[:, :4, :, :].mean(1)\n",
        "                    )\n",
        "                tail_logits = nn.Sigmoid()(\n",
        "                        attentions_scores[:, 4:8, :, :].mean(1)\n",
        "                    )\n",
        "                span_logits = nn.Sigmoid()(\n",
        "                        attentions_scores[:, 8:, :, :].mean(1)\n",
        "                    )\n",
        "            else:\n",
        "                tail_logits = nn.Sigmoid()(\n",
        "                        attentions_scores[:, :, :, :].mean(1)\n",
        "                    )\n",
        "        else:\n",
        "            # Encoding the sentence and relations in a separate manner, and add another attention layer\n",
        "            TOKEN_LEN = token_len_batch[0]\n",
        "            text_outputs = self.bert(\n",
        "                            input_ids=input_ids[:, :TOKEN_LEN],\n",
        "                            attention_mask=attention_mask[:, :TOKEN_LEN],\n",
        "                            token_type_ids=token_type_ids[:, :TOKEN_LEN],\n",
        "                            position_ids=position_ids,\n",
        "                            head_mask=head_mask,\n",
        "                            inputs_embeds=None,\n",
        "                            output_attentions=False,\n",
        "                            output_attentions_scores=False,\n",
        "                            output_hidden_states=output_hidden_states,\n",
        "                            return_dict=return_dict)\n",
        "            pred_outputs = self.bert(\n",
        "                            input_ids=input_ids[:, TOKEN_LEN:],\n",
        "                            attention_mask=attention_mask[:, TOKEN_LEN:],\n",
        "                            token_type_ids=token_type_ids[:, TOKEN_LEN:],\n",
        "                            position_ids=position_ids,\n",
        "                            head_mask=head_mask,\n",
        "                            inputs_embeds=None,\n",
        "                            output_attentions=False,\n",
        "                            output_attentions_scores=False,\n",
        "                            output_hidden_states=output_hidden_states,\n",
        "                            return_dict=return_dict)\n",
        "\n",
        "            last_hidden_state = torch.cat((text_outputs.last_hidden_state, pred_outputs.last_hidden_state), -2)\n",
        "            key_layer = self.key_linear(last_hidden_state)\n",
        "            value_layer = self.value_linear(last_hidden_state)\n",
        "            tail_logits = nn.Sigmoid()(torch.matmul(key_layer, value_layer.permute(0, 2,1)))\n",
        "\n",
        "        loss = None\n",
        "\n",
        "        if tail_label is not None:\n",
        "            tail_loss = nn.BCELoss()(tail_logits.float().reshape(-1),\n",
        "                                    tail_label.reshape(-1).float())\n",
        "            if loss is None:\n",
        "                loss = tail_loss\n",
        "            else:\n",
        "                loss += tail_loss\n",
        "        if head_label is not None:\n",
        "            head_loss = nn.BCELoss()(head_logits.float().reshape(-1),\n",
        "                                    head_label.reshape(-1).float())\n",
        "            if loss is None:\n",
        "                loss = head_loss\n",
        "            else:\n",
        "                loss += head_loss\n",
        "        if span_label is not None:\n",
        "            span_loss = nn.BCELoss()(span_logits.float().reshape(-1),\n",
        "                                    span_label.reshape(-1).float())\n",
        "            if loss is None:\n",
        "                loss = span_loss\n",
        "            else:\n",
        "                loss += span_loss\n",
        "        if tail_logits is not None:\n",
        "            tail_predictions = tail_logits > self.config.threshold\n",
        "        else:\n",
        "            tail_predictions = None\n",
        "        if head_logits is not None:\n",
        "            head_predictions = head_logits > self.config.threshold\n",
        "        else:\n",
        "            head_predictions = None\n",
        "        if span_logits is not None:\n",
        "            span_predictions = span_logits > self.config.threshold\n",
        "        else:\n",
        "            span_predictions = None\n",
        "\n",
        "        return UniRelOutput(\n",
        "            loss=loss,\n",
        "            head_preds=head_predictions,\n",
        "            tail_preds=tail_predictions,\n",
        "            span_preds=span_predictions,\n",
        "        )"
      ],
      "metadata": {
        "id": "_0wJhsI3VMY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import (BertTokenizerFast)\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "\n",
        "logger = transformers.utils.logging.get_logger(__name__)\n",
        "\n",
        "def calclulate_f1(statics_dict, prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Calculate the prec, recall and f1-score for the given state dict.\n",
        "    The state dict contains predict_num, golden_num, correct_num.\n",
        "    Reutrn a dict in the form as \"prefx-recall\": 0.99.\n",
        "    \"\"\"\n",
        "    prec, recall, f1 = 0, 0, 0\n",
        "    if statics_dict[\"c\"] != 0:\n",
        "        prec = float(statics_dict[\"c\"] / statics_dict[\"p\"])\n",
        "        recall = float(statics_dict[\"c\"] / statics_dict[\"g\"])\n",
        "        f1 = float(prec * recall) / float(prec + recall) * 2\n",
        "    return {prefix+\"-prec\": prec, prefix+\"-recall\": recall, prefix+\"-f1\": f1}\n",
        "\n",
        "def combine_dict(dicta, dictb):\n",
        "    new_dict = {}\n",
        "    for e in dicta:\n",
        "        if e not in new_dict:\n",
        "            new_dict[e] = []\n",
        "        new_dict[e] += dicta[e]\n",
        "    for e in dictb:\n",
        "        if e not in new_dict:\n",
        "            new_dict[e] = []\n",
        "        new_dict[e] += dictb[e]\n",
        "        new_dict[e] = list(set(new_dict[e]))\n",
        "    return new_dict\n",
        "\n",
        "def get_span_att(span_pred, token_len):\n",
        "    span_va = np.where(span_pred == 1)\n",
        "    t2_span = dict()\n",
        "    h2_span = dict()\n",
        "    for s, e in zip(span_va[0], span_va[1]):\n",
        "        # if s > token_len or e > token_len or s == 0 or e == 0:\n",
        "        if s > token_len or e > token_len:\n",
        "            continue\n",
        "        if e < s:\n",
        "            continue\n",
        "        if e not in t2_span:\n",
        "            t2_span[e] = []\n",
        "        if s not in h2_span:\n",
        "            h2_span[s] = []\n",
        "        s = int(s)\n",
        "        e = int(e)\n",
        "        t2_span[e].append((s,e))\n",
        "        h2_span[s].append((s,e))\n",
        "    return  h2_span, t2_span\n",
        "\n",
        "def get_e2r(e2r_pred, token_len):\n",
        "    \"\"\"\n",
        "    Extract entity-relation (subject-relation) and entity-entity interactions from given Attention Matrix.\n",
        "    Only Extract the upper-right triangle, so should input transpose of the original\n",
        "    Attention Matrix to extract relation-entity (relation-object) interactions.\n",
        "    \"\"\"\n",
        "    e2r = {}\n",
        "    tok_tok = set()\n",
        "    e_va = np.where(e2r_pred == 1)\n",
        "    for h, r in zip(e_va[0], e_va[1]):\n",
        "        h = int(h)\n",
        "        r = int(r)\n",
        "        if h == 0 or r == 0 or r == token_len+1 or h > token_len:\n",
        "            continue\n",
        "        # Entity-Entity\n",
        "        if r < token_len+1:\n",
        "            tok_tok.add((h,r))\n",
        "        # Entity-Relation\n",
        "        else:\n",
        "            r = int(r-token_len-2)\n",
        "            if h not in e2r:\n",
        "                e2r[h] = []\n",
        "            e2r[h].append(r)\n",
        "    return e2r, tok_tok\n",
        "\n",
        "def unirel_extractor(tokenizer,\n",
        "                   dataset,\n",
        "                   predictions,\n",
        "                   path,\n",
        "                   ):\n",
        "    \"\"\"\n",
        "    Extractor triples from the modeled Attention matrix\n",
        "    \"\"\"\n",
        "    # Minus the [cls] and [sep]\n",
        "    token_len = dataset.max_length - 2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    state_dict = {\"p\": 0, \"c\": 0, \"g\": 0}\n",
        "    e2e_state_dict = {\"p\": 0, \"c\": 0, \"g\": 0}\n",
        "    e2e_tail_state_dict = {\"p\": 0, \"c\": 0, \"g\": 0}\n",
        "    e2e_plain_state_dict = {\"p\": 0, \"c\": 0, \"g\": 0}\n",
        "    h2r_state_dict = {\"p\": 0, \"c\": 0, \"g\": 0}\n",
        "    t2r_state_dict = {\"p\": 0, \"c\": 0, \"g\": 0}\n",
        "    idx2pred = dataset.data_processor.idx2pred\n",
        "    extract_data = []\n",
        "    path = os.path.join(path, dataset.mode + '_predict_sard.json')\n",
        "    tail_preds = predictions.predictions\n",
        "    tail_labels = predictions.label_ids\n",
        "    # NOTE: This is only for test!\n",
        "    # tail_preds, head_preds, span_preds = predictions.label_ids\n",
        "    curr_data_idx = 0\n",
        "    for tail_pred, tail_label in zip(tail_preds, tail_labels):\n",
        "        input_ids = dataset[curr_data_idx][\"input_ids\"]\n",
        "        text = dataset.texts[curr_data_idx]\n",
        "        spo_list = dataset.spo_lists[curr_data_idx]\n",
        "        spo_span_list = dataset.spo_span_lists[curr_data_idx]\n",
        "        gold_spo_text = set()\n",
        "        gold_ee_list = set()\n",
        "        gold_plain_ee_list = set()\n",
        "        gold_sr_list = {}\n",
        "        gold_or_list = {}\n",
        "        gold_er_list = {}\n",
        "        gold_entity_list = set()\n",
        "        gold_ee_tail_list = set()\n",
        "        # Extract golden triples with same tokenizer\n",
        "        for spo in spo_span_list:\n",
        "            rel_str = dataset.data_processor.idx2pred[spo[1]]\n",
        "            left_str = tokenizer.decode(input_ids[spo[0][1]])\n",
        "            right_str = tokenizer.decode(input_ids[spo[2][1]])\n",
        "            gold_ee_list.add((spo[0][1], spo[2][1]))\n",
        "            gold_plain_ee_list.add((spo[0][1], spo[2][1]))\n",
        "            gold_plain_ee_list.add((spo[2][1], spo[0][1]))\n",
        "            gold_ee_tail_list.add((spo[0][1], spo[2][1]))\n",
        "            # gold_ee_list.add((spo[2][1], spo[0][1]))\n",
        "            gold_entity_list.add(spo[0][1])\n",
        "            gold_entity_list.add(spo[2][1])\n",
        "            if spo[0][1] not in gold_sr_list:\n",
        "                gold_sr_list[spo[0][1]] = set()\n",
        "            gold_sr_list[spo[0][1]].add(spo[1])\n",
        "            if spo[2][1] not in gold_or_list:\n",
        "                gold_or_list[spo[2][1]] = set()\n",
        "            gold_or_list[spo[2][1]].add(spo[1])\n",
        "            if spo[0][1] not in gold_er_list:\n",
        "                gold_er_list[spo[0][1]] = set()\n",
        "            if spo[2][1] not in gold_er_list:\n",
        "                gold_er_list[spo[2][1]] = set()\n",
        "            gold_er_list[spo[0][1]].add(spo[1])\n",
        "            gold_er_list[spo[2][1]].add(spo[1])\n",
        "\n",
        "            gold_spo_text.add((left_str, rel_str, right_str))\n",
        "\n",
        "        curr_data_idx += 1\n",
        "        pred_spo_text = set()\n",
        "        pred_spo_span_list = set()\n",
        "        pred_ee_list = set()\n",
        "        pred_ee_tail_list = set()\n",
        "        pred_plain_ee_list = set()\n",
        "        # h2r: subject(head) - relation, e2e: entity - entity\n",
        "        e_h2r, e2e = get_e2r(tail_pred, token_len)\n",
        "        # t2r: object(tail) - relation\n",
        "        e_t2r, t_e2e = get_e2r(tail_pred.T, token_len)\n",
        "        # For each possible entity pair\n",
        "        for left, right in e2e:\n",
        "            # Consider both directions\n",
        "            for l,r in [(left, right), (right, left)]:\n",
        "                pred_plain_ee_list.add((l,r))\n",
        "                # Find mutual relations\n",
        "                if l in e_h2r and r in e_t2r:\n",
        "                    common_rels = set(e_h2r[l]) & set(e_t2r[r])\n",
        "                    for rel in common_rels:\n",
        "                        pred_ee_list.add((l, r))\n",
        "                        pred_spo_span_list.add((\n",
        "                            l, rel, r\n",
        "                        ))\n",
        "                        pred_spo_text.add((\n",
        "                            tokenizer.decode(input_ids[l]),\n",
        "                            idx2pred[rel],\n",
        "                            tokenizer.decode(input_ids[r])\n",
        "                        ))\n",
        "        state_dict[\"p\"] += len(pred_spo_text)\n",
        "        state_dict[\"g\"] += len(gold_spo_text)\n",
        "        state_dict[\"c\"] += len(pred_spo_text & gold_spo_text)\n",
        "\n",
        "        e2e_state_dict[\"p\"] += len(pred_ee_list)\n",
        "        e2e_state_dict[\"g\"] += len(gold_ee_list)\n",
        "        e2e_state_dict[\"c\"] += len(set(pred_ee_list) & set(gold_ee_list))\n",
        "\n",
        "        e2e_plain_state_dict[\"p\"] += len(pred_plain_ee_list)\n",
        "        e2e_plain_state_dict[\"g\"] += len(gold_plain_ee_list)\n",
        "        e2e_plain_state_dict[\"c\"] += len(set(pred_plain_ee_list) & set(gold_plain_ee_list))\n",
        "\n",
        "        e2e_tail_state_dict[\"p\"] += len(pred_ee_tail_list)\n",
        "        e2e_tail_state_dict[\"g\"] += len(gold_ee_tail_list)\n",
        "        e2e_tail_state_dict[\"c\"] += len(set(pred_ee_tail_list) & set(gold_ee_tail_list))\n",
        "        for e in e_h2r:\n",
        "            if e in gold_sr_list:\n",
        "                h2r_state_dict[\"p\"] += len(e_h2r[e])\n",
        "                h2r_state_dict[\"g\"] += len(gold_sr_list[e])\n",
        "                h2r_state_dict[\"c\"] += len(set(e_h2r[e]) & set(gold_sr_list[e]))\n",
        "        for e in e_t2r:\n",
        "            if e in gold_or_list:\n",
        "                t2r_state_dict[\"p\"] += len(e_t2r[e])\n",
        "                t2r_state_dict[\"g\"] += len(gold_or_list[e])\n",
        "                t2r_state_dict[\"c\"] += len(set(e_t2r[e]) & set(gold_or_list[e]))\n",
        "\n",
        "\n",
        "        extract_data.append({\n",
        "            \"text\": text,\n",
        "            \"gold_spo_list\": list(spo_list),\n",
        "            \"pred_spo_list\": list(pred_spo_text),\n",
        "            \"gold_spo_tail_list\": list(gold_spo_text),\n",
        "            \"pred_spo_span_list\": list(pred_spo_span_list),\n",
        "            \"gold_spo_span_list\": list(spo_span_list),\n",
        "        })\n",
        "    # print(calclulate_f1(state_dict))\n",
        "    all_metirc_results = calclulate_f1(state_dict, 'all')\n",
        "    print(f\"\\nall:  {state_dict} \\n {calclulate_f1(state_dict, 'all')}\")\n",
        "    print(f\"\\ne2e:  {e2e_state_dict} \\n {calclulate_f1(e2e_state_dict, 'e2e')}\")\n",
        "    print(f\"\\nh2r:  {h2r_state_dict} \\n {calclulate_f1(h2r_state_dict, 'h2r')}\")\n",
        "    print(f\"\\nt2r:  {t2r_state_dict} \\n {calclulate_f1(t2r_state_dict, 't2r')}\")\n",
        "    print(f\"\\ne2e_tail:  {e2e_tail_state_dict} \\n {calclulate_f1(e2e_tail_state_dict, 'e2e_tail')}\")\n",
        "    print(f\"\\ne2e without rel:  {e2e_plain_state_dict} \\n {calclulate_f1(e2e_plain_state_dict, 'e2e_plain')}\")\n",
        "    logger.info(f\"\\nall:  {calclulate_f1(state_dict, 'all')}\")\n",
        "    logger.info(f\"\\ne2e:  {calclulate_f1(e2e_state_dict, 'e2e')}\")\n",
        "    logger.info(f\"\\nh2r:  {calclulate_f1(h2r_state_dict, 'h2r')}\")\n",
        "    logger.info(f\"\\nt2r:  {calclulate_f1(t2r_state_dict, 't2r')}\")\n",
        "    logger.info(f\"\\ne2e_tail:  {calclulate_f1(e2e_tail_state_dict, 'e2e_tail')}\")\n",
        "    logger.info(f\"\\ne2e without rel:  {e2e_plain_state_dict} \\n {calclulate_f1(e2e_plain_state_dict, 'e2e_plain')}\")\n",
        "    with open(path, \"w\") as wp:\n",
        "        json.dump(extract_data, wp, indent=2, ensure_ascii=False)\n",
        "    return all_metirc_results[\"all-prec\"], all_metirc_results[\"all-recall\"], all_metirc_results[\"all-f1\"]\n",
        "\n",
        "\n",
        "def unirel_span_extractor(tokenizer,\n",
        "                   dataset,\n",
        "                   predictions,\n",
        "                   path,\n",
        "                   ):\n",
        "    \"\"\"\n",
        "    Extractor triples from the modeled Attention matrix\n",
        "    \"\"\"\n",
        "    # Minus the [cls] and [sep]\n",
        "    token_len = dataset.max_length - 2\n",
        "\n",
        "    state_dict = {\"p\": 0, \"c\": 0, \"g\": 0}\n",
        "    e2e_state_dict = {\"p\": 0, \"c\": 0, \"g\": 0}\n",
        "    e2e_tail_state_dict = {\"p\": 0, \"c\": 0, \"g\": 0}\n",
        "    e2e_plain_state_dict = {\"p\": 0, \"c\": 0, \"g\": 0}\n",
        "    h2r_state_dict = {\"p\": 0, \"c\": 0, \"g\": 0}\n",
        "    t2r_state_dict = {\"p\": 0, \"c\": 0, \"g\": 0}\n",
        "    idx2pred = dataset.data_processor.idx2pred\n",
        "    extract_data = []\n",
        "    path = os.path.join(path, dataset.mode + '_predict_sard.json')\n",
        "    head_preds, tail_preds, span_preds = predictions.predictions\n",
        "    head_labels, tail_labels, span_labels = predictions.label_ids\n",
        "    # NOTE: This is only for test!\n",
        "    # head_preds, tail_preds, span_preds = predictions.label_ids\n",
        "    curr_data_idx = 0\n",
        "    # for tail_pred, tail_label in zip(tail_preds, tail_labels):\n",
        "    for head_pred, tail_pred, span_pred in zip(head_preds, tail_preds, span_preds):\n",
        "        input_ids = dataset[curr_data_idx][\"input_ids\"]\n",
        "        text = dataset.texts[curr_data_idx]\n",
        "        spo_list = dataset.spo_lists[curr_data_idx]\n",
        "        spo_span_list = dataset.spo_span_lists[curr_data_idx]\n",
        "        gold_spo_text = set()\n",
        "        # Extract golden triples with same tokenizer\n",
        "        for spo in spo_span_list:\n",
        "            rel_str = dataset.data_processor.idx2pred[spo[1]]\n",
        "            left_str = tokenizer.decode(input_ids[spo[0][0]+1:spo[0][1]+1])\n",
        "            right_str = tokenizer.decode(input_ids[spo[2][0]+1:spo[2][1]+1])\n",
        "            gold_spo_text.add((left_str, rel_str, right_str))\n",
        "\n",
        "        curr_data_idx += 1\n",
        "\n",
        "        pred_spo_text = set()\n",
        "        s_h2r, s2s = get_e2r(head_pred, token_len)\n",
        "        s_t2r, _ = get_e2r(head_pred.T, token_len)\n",
        "        e_h2r, e2e = get_e2r(tail_pred, token_len)\n",
        "        e_t2r, _ = get_e2r(tail_pred.T, token_len)\n",
        "        start2span, end2span = get_span_att(span_pred, token_len)\n",
        "        for l, r in e2e:\n",
        "            if l not in e_h2r or r not in e_t2r:\n",
        "                continue\n",
        "            if l not in end2span or r not in end2span:\n",
        "                continue\n",
        "            l_spans, r_spans = end2span[l], end2span[r]\n",
        "            for l_span in l_spans:\n",
        "                for r_span in r_spans:\n",
        "                    l_s, r_s = l_span[0], r_span[0]\n",
        "                    if (l_s, r_s) not in s2s:\n",
        "                        continue\n",
        "                    if l_s not in s_h2r or r_s not in s_t2r:\n",
        "                        continue\n",
        "                    common_rels = set(s_h2r[l_s])& set(s_t2r[r_s]) & set(e_h2r[l]) & set(e_t2r[r])\n",
        "                    # l_span_new = (l_span[0]+1, l_span[1])\n",
        "                    # r_span_new = (r_span[0]+1, r_span[1])\n",
        "                    l_span_new = (l_span[0], l_span[1])\n",
        "                    r_span_new = (r_span[0], r_span[1])\n",
        "                    for rel in common_rels:\n",
        "                        pred_spo_text.add((\n",
        "                            tokenizer.decode(input_ids[l_span_new[0]:l_span_new[1]+1]),\n",
        "                            idx2pred[rel],\n",
        "                            tokenizer.decode(input_ids[r_span_new[0]:r_span_new[1]+1])\n",
        "                        ))\n",
        "\n",
        "        state_dict[\"p\"] += len(pred_spo_text)\n",
        "        state_dict[\"g\"] += len(gold_spo_text)\n",
        "        state_dict[\"c\"] += len(pred_spo_text & gold_spo_text)\n",
        "        # if len(pred_spo_text & gold_spo_text) != len(gold_spo_text):\n",
        "        #     print(\"problem\")\n",
        "\n",
        "\n",
        "\n",
        "        extract_data.append({\n",
        "            \"text\": text,\n",
        "            \"gold_spo_list\": list(spo_list),\n",
        "            \"pred_spo_list\": list(pred_spo_text),\n",
        "        })\n",
        "    # print(calclulate_f1(state_dict))\n",
        "    all_metirc_results = calclulate_f1(state_dict, 'all')\n",
        "    print(f\"\\nall:  {state_dict} \\n {calclulate_f1(state_dict, 'all')}\")\n",
        "    with open(path, \"w\") as wp:\n",
        "        json.dump(extract_data, wp, indent=2, ensure_ascii=False)\n",
        "    return all_metirc_results[\"all-prec\"], all_metirc_results[\"all-recall\"], all_metirc_results[\"all-f1\"]"
      ],
      "metadata": {
        "id": "cP_YoU44ak8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, matthews_corrcoef, f1_score\n",
        "from transformers import EvalPrediction, set_seed\n",
        "\n",
        "def calclulate_f1(statics_dict):\n",
        "    prec, recall, f1 = 0, 0, 0\n",
        "    if statics_dict[\"c\"] != 0:\n",
        "        prec = float(statics_dict[\"c\"] / statics_dict[\"p\"])\n",
        "        recall = float(statics_dict[\"c\"] / statics_dict[\"g\"])\n",
        "        f1 = float(prec * recall) / float(prec + recall) * 2\n",
        "    return {\"prec\": prec, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "def unirel_metric(p: EvalPrediction):\n",
        "    tail_labels = p.label_ids\n",
        "    tail_preds = p.predictions\n",
        "    tail_acc, tail_recall, tail_f1, _ = precision_recall_fscore_support(\n",
        "        y_pred=tail_preds.reshape(-1),\n",
        "        y_true=tail_labels.reshape(-1),\n",
        "        labels=[1],\n",
        "        average='micro')\n",
        "\n",
        "    return {\n",
        "        \"acc\": tail_acc,\n",
        "        \"recall\": tail_recall,\n",
        "        \"f1\": tail_f1,\n",
        "    }\n",
        "\n",
        "def unirel_span_metric(p: EvalPrediction):\n",
        "    head_labels, tail_labels, span_labels = p.label_ids\n",
        "    head_preds, tail_preds, span_preds = p.predictions\n",
        "    head_acc, head_recall, head_f1, _ = precision_recall_fscore_support(\n",
        "        y_pred=head_preds.reshape(-1),\n",
        "        y_true=head_labels.reshape(-1),\n",
        "        labels=[1],\n",
        "        average='micro')\n",
        "    tail_acc, tail_recall, tail_f1, _ = precision_recall_fscore_support(\n",
        "        y_pred=tail_preds.reshape(-1),\n",
        "        y_true=tail_labels.reshape(-1),\n",
        "        labels=[1],\n",
        "        average='micro')\n",
        "    span_acc, span_recall, span_f1, _ = precision_recall_fscore_support(\n",
        "        y_pred=span_preds.reshape(-1),\n",
        "        y_true=span_labels.reshape(-1),\n",
        "        labels=[1],\n",
        "        average='micro')\n",
        "    return {\n",
        "        \"head_acc\": head_acc,\n",
        "        \"head_recall\": head_recall,\n",
        "        \"head_f1\": head_f1,\n",
        "        \"tail_acc\": tail_acc,\n",
        "        \"tail_recall\": tail_recall,\n",
        "        \"tail_f1\": tail_f1,\n",
        "        \"span_acc\": span_acc,\n",
        "        \"span_recall\": span_recall,\n",
        "        \"span_f1\": span_f1,\n",
        "    }"
      ],
      "metadata": {
        "id": "AqnGDEcsVhvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from transformers import (BertTokenizerFast)\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "class UniRel:\n",
        "    def __init__(self, model_path, max_length=128, dataset_name=\"nyt\") -> None:\n",
        "        self.model = UniRelModel.from_pretrained(model_path, attn_implementation=\"eager\")\n",
        "        added_token = [f\"[unused{i}]\" for i in range(1, 17)]\n",
        "        self.tokenizer = BertTokenizerFast.from_pretrained(\n",
        "            \"bert-base-cased\", additional_special_tokens=added_token, do_basic_tokenize=False)\n",
        "        self.max_length = max_length\n",
        "        self.max_length = max_length\n",
        "        self._get_pred_str(dataset_name)\n",
        "\n",
        "\n",
        "    def _get_pred_str(self, dataset_name):\n",
        "        self.pred2text = None\n",
        "        if dataset_name == \"nyt\":\n",
        "            self.pred2text=nyt_rel2text\n",
        "        elif dataset_name == \"nyt_star\":\n",
        "            self.pred2text=nyt_rel2text\n",
        "        elif dataset_name == \"webnlg\":\n",
        "            self.pred2text=webnlg_rel2text\n",
        "            cnt = 1\n",
        "            exist_value=[]\n",
        "            # Some hard to convert relation directly use [unused]\n",
        "            for k in self.pred2text:\n",
        "                v = self.pred2text[k]\n",
        "                if isinstance(v, int):\n",
        "                    self.pred2text[k] = f\"[unused{cnt}]\"\n",
        "                    cnt += 1\n",
        "                    continue\n",
        "                ids = self.tokenizer(v)\n",
        "                if len(ids[\"input_ids\"]) != 3:\n",
        "                    print(k, \"   \", v)\n",
        "                if v in exist_value:\n",
        "                    print(\"exist\", k, \"  \", v)\n",
        "                else:\n",
        "                    exist_value.append(v)\n",
        "        elif dataset_name == \"webnlg_star\":\n",
        "            self.pred2text = webnlg_rel2text\n",
        "            cnt = 1\n",
        "            exist_value=[]\n",
        "            for k in self.pred2text:\n",
        "                v = self.pred2text[k]\n",
        "                if isinstance(v, int):\n",
        "                    self.pred2text[k] = f\"[unused{cnt}]\"\n",
        "                    cnt += 1\n",
        "                    continue\n",
        "                ids = self.tokenizer(v)\n",
        "                if len(ids[\"input_ids\"]) != 3:\n",
        "                    print(k, \"   \", v)\n",
        "                if v in exist_value:\n",
        "                    print(\"exist\", k, \"  \", v)\n",
        "                else:\n",
        "                    exist_value.append(v)\n",
        "            # self.pred2text = {key: \"[unused\"+str(i+1)+\"]\" for i, key in enumerate(self.label2id.keys())}\n",
        "        else:\n",
        "            print(\"dataset name error\")\n",
        "            exit(0)\n",
        "        self.pred_str = \"\"\n",
        "        self.max_label_len = 1\n",
        "        self.pred2idx = {}\n",
        "        idx = 0\n",
        "        for k in self.pred2text:\n",
        "            self.pred2idx[k] = idx\n",
        "            self.pred_str += self.pred2text[k] + \" \"\n",
        "            idx += 1\n",
        "        self.num_rels = len(self.pred2text.keys())\n",
        "        self.idx2pred = {value: key for key, value in self.pred2idx.items()}\n",
        "        self.pred_str = self.pred_str[:-1]\n",
        "        self.pred_inputs = self.tokenizer.encode_plus(self.pred_str,\n",
        "                                                 add_special_tokens=False)\n",
        "\n",
        "    def _data_process(self, text):\n",
        "        # text could be a list of sentences or a single sentence\n",
        "        if isinstance(text, str):\n",
        "            text = [text]\n",
        "        inputs = self.tokenizer.batch_encode_plus(text, max_length=self.max_length, padding=\"max_length\", truncation=True)\n",
        "        batched_input_ids = []\n",
        "        batched_attention_mask = []\n",
        "        batched_token_type_ids = []\n",
        "        for b_input_ids, b_attention_mask, b_token_type_ids in zip(inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs[\"token_type_ids\"]):\n",
        "            input_ids = b_input_ids + self.pred_inputs[\"input_ids\"]\n",
        "            sep_idx = b_input_ids.index(self.tokenizer.sep_token_id)\n",
        "            input_ids[sep_idx] = self.tokenizer.pad_token_id\n",
        "            attention_mask = b_attention_mask + [1]*self.num_rels\n",
        "            attention_mask[sep_idx] = 0\n",
        "            token_type_ids = b_token_type_ids + [1]*self.num_rels\n",
        "            batched_input_ids.append(input_ids)\n",
        "            batched_attention_mask.append(attention_mask)\n",
        "            batched_token_type_ids.append(token_type_ids)\n",
        "        return batched_input_ids, batched_attention_mask, batched_token_type_ids\n",
        "\n",
        "\n",
        "    def _get_e2r(self, e2r_pred):\n",
        "        \"\"\"\n",
        "        Extract entity-relation (subject-relation) and entity-entity interactions from given Attention Matrix.\n",
        "        Only Extract the upper-right triangle, so should input transpose of the original\n",
        "        Attention Matrix to extract relation-entity (relation-object) interactions.\n",
        "        \"\"\"\n",
        "        token_len = self.max_length-2\n",
        "        e2r = {}\n",
        "        tok_tok = set()\n",
        "        e_va = np.where(e2r_pred == 1)\n",
        "        for h, r in zip(e_va[0], e_va[1]):\n",
        "            h = int(h)\n",
        "            r = int(r)\n",
        "            if h == 0 or r == 0 or r == token_len+1 or h > token_len:\n",
        "                continue\n",
        "            # Entity-Entity\n",
        "            if r < token_len+1:\n",
        "                tok_tok.add((h,r))\n",
        "            # Entity-Relation\n",
        "            else:\n",
        "                r = int(r-token_len-2)\n",
        "                if h not in e2r:\n",
        "                    e2r[h] = []\n",
        "                e2r[h].append(r)\n",
        "        return e2r, tok_tok\n",
        "\n",
        "    def _get_span_att(self, span_pred):\n",
        "        token_len = self.max_length-2\n",
        "        span_va = np.where(span_pred == 1)\n",
        "        t2_span = dict()\n",
        "        h2_span = dict()\n",
        "        for s, e in zip(span_va[0], span_va[1]):\n",
        "            # if s > token_len or e > token_len or s == 0 or e == 0:\n",
        "            if s > token_len or e > token_len:\n",
        "                continue\n",
        "            if e < s:\n",
        "                continue\n",
        "            if e not in t2_span:\n",
        "                t2_span[e] = []\n",
        "            if s not in h2_span:\n",
        "                h2_span[s] = []\n",
        "            s = int(s)\n",
        "            e = int(e)\n",
        "            t2_span[e].append((s,e))\n",
        "            h2_span[s].append((s,e))\n",
        "        return  h2_span, t2_span\n",
        "\n",
        "    def _extractor(self, outputs, input_ids_list):\n",
        "        preds_list = []\n",
        "        for head_pred, tail_pred, span_pred, input_ids in zip(outputs[\"head_preds\"], outputs[\"tail_preds\"], outputs[\"span_preds\"], input_ids_list):\n",
        "            pred_spo_text = set()\n",
        "            s_h2r, s2s = self._get_e2r(head_pred)\n",
        "            s_t2r, _ = self._get_e2r(head_pred.T)\n",
        "            e_h2r, e2e = self._get_e2r(tail_pred)\n",
        "            e_t2r, _ = self._get_e2r(tail_pred.T)\n",
        "            start2span, end2span = self._get_span_att(span_pred)\n",
        "            for l, r in e2e:\n",
        "                if l not in e_h2r or r not in e_t2r:\n",
        "                    continue\n",
        "                if l not in end2span or r not in end2span:\n",
        "                    continue\n",
        "                l_spans, r_spans = end2span[l], end2span[r]\n",
        "                for l_span in l_spans:\n",
        "                    for r_span in r_spans:\n",
        "                        l_s, r_s = l_span[0], r_span[0]\n",
        "                        if (l_s, r_s) not in s2s:\n",
        "                            continue\n",
        "                        if l_s not in s_h2r or r_s not in s_t2r:\n",
        "                            continue\n",
        "                        common_rels = set(s_h2r[l_s])& set(s_t2r[r_s]) & set(e_h2r[l]) & set(e_t2r[r])\n",
        "                        # l_span_new = (l_span[0]+1, l_span[1])\n",
        "                        # r_span_new = (r_span[0]+1, r_span[1])\n",
        "                        l_span_new = (l_span[0], l_span[1])\n",
        "                        r_span_new = (r_span[0], r_span[1])\n",
        "                        for rel in common_rels:\n",
        "                            pred_spo_text.add((\n",
        "                                self.tokenizer.decode(input_ids[l_span_new[0]:l_span_new[1]+1]),\n",
        "                                self.idx2pred[rel],\n",
        "                                self.tokenizer.decode(input_ids[r_span_new[0]:r_span_new[1]+1])\n",
        "                            ))\n",
        "            preds_list.append(list(pred_spo_text))\n",
        "        return preds_list\n",
        "\n",
        "    def predict(self, text):\n",
        "        input_ids, attention_mask, token_type_ids = self._data_process(text)\n",
        "        if isinstance(input_ids, list):\n",
        "            input_ids = torch.tensor(input_ids)\n",
        "            attention_mask = torch.tensor(attention_mask)\n",
        "            token_type_ids = torch.tensor(token_type_ids)\n",
        "        else:\n",
        "            input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
        "            attention_mask = torch.tensor(attention_mask).unsqueeze(0)\n",
        "            token_type_ids = torch.tensor(token_type_ids).unsqueeze(0)\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids, attention_mask, token_type_ids)\n",
        "            results = self._extractor(outputs, input_ids)\n",
        "        return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = \"/content/drive/MyDrive/unirel\"\n",
        "    unirel = UniRel(model_path, dataset_name=\"nyt\")\n",
        "\n",
        "    print(unirel.predict([\"\"\"\n",
        "\n",
        "    Joseph Robinette Biden Jr. (/badn/; born November 20, 1942) is an American politician who is the 46th and current president of the United States.\n",
        "    A member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama, and represented Delaware in the United States Senate from 1973 to 2009.\n",
        "    Biden was born and raised in Scranton, Pennsylvania, and moved with his family to Delaware in 1953 when he was ten years old.\n",
        "    He studied at the University of Delaware before earning his law degree from Syracuse University.\n",
        "    He was elected to the New Castle County Council in 1970 and became the sixth-youngest senator in U.S. history after he was elected to the United States Senate from Delaware in 1972, at age 29.\n",
        "    Biden was the chair or ranking member of the Senate Foreign Relations Committee for 12 years.\n",
        "    He also chaired the Senate Judiciary Committee from 1987 to 1995; led the effort to pass the Violent Crime Control and Law Enforcement Act and the Violence Against Women Act; and oversaw six U.S. Supreme Court confirmation hearings, including the contentious hearings for Robert Bork and Clarence Thomas.\n",
        "    Biden ran unsuccessfully for the Democratic presidential nomination in 1988 and 2008, before becoming Obama's vice president after they won the 2008 presidential election.\n",
        "    During his two terms as vice president, Biden frequently represented the administration in negotiations with congressional Republicans and was a close counselor to President Obama.\n",
        "    Biden and his running mate, Kamala Harris, defeated incumbent Donald Trump in the 2020 presidential election.\n",
        "    Upon inauguration, he became the oldest president in U.S. history and the first to have a female vice president.\n",
        "    Biden signed the American Rescue Plan Act to help the U.S. recover from the COVID-19 pandemic and subsequent recession.\n",
        "    He proposed the American Jobs Plan, aspects of which were incorporated into the bipartisan Infrastructure Investment and Jobs Act.\n",
        "    He proposed the American Families Plan, which was merged with other aspects of the American Jobs Plan into the proposed Build Back Better Act.\n",
        "    After facing opposition in the Senate, the Build Back Better Act's size was reduced and it was comprehensively reworked into the Inflation Reduction Act of 2022, covering deficit reduction, climate change, healthcare, and tax reform.\n",
        "    Biden appointed Ketanji Brown Jackson to the Supreme Court. In foreign policy, he restored the U.S. into the Paris Agreement on climate change.\n",
        "    He completed the withdrawal of U.S. troops from Afghanistan, during which the Afghan government collapsed and the Taliban seized control.\n",
        "    He responded to the 2022 Russian invasion of Ukraine by imposing sanctions on Russia and authorizing foreign aid and weapons shipments to Ukraine. (en)\n",
        "\n",
        "\"\"\"]))\n",
        "\n",
        "    print(unirel.predict(\"In perhaps the most ambitious Mekong cruise attempt, Impulse Tourism, an operator based in Chiang Mai, Thailand, is organizing an expedition starting in November in Jinghong, a small city in the Yunnan province in China.\"))\n",
        "    print(unirel.predict(\"Adisham Hall in Sri Lanka was constructed between 1927 and 1931 at St Benedicts Monastery , Adisham , Haputhale , Sri Lanka in the Tudor and Jacobean style of architecture\"))\n",
        "    print(\"end\")"
      ],
      "metadata": {
        "id": "li329f5eagMH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "458c8d3a-0a31-49f1-9d40-281cf70f3293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('Pennsylvania', '/location/location/contains', 'Scranton')]]\n",
            "[[('China', '/location/country/administrative_divisions', 'Yunnan'), ('Thailand', '/location/location/contains', 'Chiang Mai'), ('China', '/location/location/contains', 'Jinghong'), ('Yunnan', '/location/administrative_division/country', 'China'), ('China', '/location/location/contains', 'Yunnan')]]\n",
            "[[('Sri Lanka', '/location/location/contains', 'Haputhale')]]\n",
            "end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BzDXMUZv4ZkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ET4pomQbEJNP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}